{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Livrable Projet DATA SCIENCE</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "L'entreprise TouNum est une entreprise de numérisation de documents. Elle prospose différents services dont la numérisation de base de document papier pour les entreprises clientes. TouNum veut optimiser et rendre intelligent ce processus de scanning en incluant des outils de Machine Learning. Le gain de temps serait important aux vues des nombreuses données que l'entreprise doit scanner et étiqueter.\n",
    "Pour cela, TouNum fait appel à CESI pour réaliser cette prestation.\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est que l'équipe de data scientist de CESI réalise cette solution visant à analyser des photographies pour en déterminer une légende descriptive de manière automatique. Il faudra également améliorer la qualité des images scannées ayant des qualités variables (parfois floues, ou bruitées).\n",
    "\n",
    "<img src=\"imageSrc/caption image.PNG\"/>\n",
    "\n",
    "### Enjeux\n",
    "\n",
    "TouNum devait trier et étiqueter chaque document scanné. La solution délivré par CESI permet l'automatisation de ces tâches en faisant donc gagner un temps non négligeable. Elle va donc pouvoir réaliser plus de contrats et augmenter la satisfaction client.\n",
    "\n",
    "### Contraintes techniques\n",
    "\n",
    "L'implémentation des algorithmes doit être réaliser sur Python, notamment les librairies Scikit et TensorFlow. La librairie Pandas doit être utilisé pour manipuler le dataset et ImageIO pour le charger. NumPy et MatPlotLib seront nécessaire pour le calcul scientifique et la modélisation.\n",
    "\n",
    "Le programme à livrer devra respecter le workflow suivant :\n",
    "\n",
    "<img src=\"imageSrc/workflow.PNG\"/>\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "La classification d'image se fera à l'aide de réseaux de neurones. Cette dernière doit distinguer les photos d'un autre documents, tel que schémas, textes scannés, voir peintures.\n",
    "TouNoum possède un dataset rempli d'images divers pour entrainer le réseau de neurones.\n",
    "\n",
    "#### Prétraitement\n",
    "\n",
    "Le prétraitement dois utiliser des filtres convolutifs afin d'améliorer la qualité. Il doit établir un compromis entre débruitage et affutage.\n",
    "\n",
    "#### Captionning\n",
    "\n",
    "Le Captionning devra légender automatiquement les images. Il utilisera deux techniques de Machine Learning : les réseaux de neurones convolutifs (CNN) pour prétraiter l'image en identifiant les zones d’intérêt, et les réseaux de neurones récurrents (RNN) pour générer les étiquettes. Il faudra être vigilant quant aux ressources RAM. Un dataset d'étiquetage classique est disponible pour l’apprentissage supervisé.\n",
    "\n",
    "### Livrable\n",
    "\n",
    "La solution doit sous forme de notebook Jupiter entièrement automatisé. Il doit être conçu pour être faciliter mis en production et maintenance.\n",
    "Il faut démontrer la pertinence du modèle de manière rigoureuse et pédagogique.\n",
    "\n",
    "#### Jalons\n",
    "\n",
    "CESI devra dois rendre le prototype complet et fonctionnel du programme pour le 23 janvier. \n",
    "TouNum exige également 3 dates de rendu pour suivre la bonne avancé du projet.\n",
    "<ul>\n",
    "    <li>18/12/20 : Prétraitement d'image</li>\n",
    "    <li>15/01/21 : Classification binaire</li>\n",
    "    <li>20/01/21 : Captioning d'images</li>\n",
    "    <li>22/01/21 : Démonstration </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable 1 - Prétraitement (denoising/sharpening…)\n",
    "\n",
    "Le but est de traiter un ensemble de photographies afin de les rendre mieux traitables par les algorithmes de Machine Learning. Il y a deux traitements à réaliser : le débruitage, et l’affutage. Vous devrez produire un notebook Jupyter explicitant ces étapes de prétraitement, et leurs performances. Ces algorithmes s’appuieront sur des notions assez simples autour des filtres de convolution, et les appliqueront pour améliorer la qualité de l’image. Il faudra notamment décider d’un compromis entre dé-bruitage et affutage.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code de chargement du fichier.</li>\n",
    "    <li>Le code du débruitage sur un sous-ensemble d’images bruitées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>Le code de l’affutage sur un sous-ensembles d’images floutées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>\n",
    "        Une étude de cas explicitant les compromis entre ces deux opérations. Cette partie du livrable doit inclure le bruitage d’images et montrer la perte de détails, ou l’affutage d’images et montrer l’apparition du bruit.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/12/2020</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Importation des librairies utilisées*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Check if imageio package is installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio\n",
    "    \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if cikit-image package is installed\n",
    "try:\n",
    "    import skimage\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "from skimage import io\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "# Check if opencv-python package is installed\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    !pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Check if pandas package is installed\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "# Check if tensorflow package is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import tensorflow_datasets\n",
    "except ImportError:\n",
    "    !pip install tensorflow_datasets    \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    !pip install keras    \n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Livrable 3 \n",
    "import json\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Lib to move images\n",
    "import shutil\n",
    "\n",
    "# Check if tqdm package is installed\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    !pip install -q tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Chemins physiques*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_dataset_path = \"./Dataset/1/dataset/Blurry/\"\n",
    "noisy_dataset_path = \"./Dataset/1/dataset/Noisy/\"\n",
    "\n",
    "deblured_dataset_path = \"./Dataset/1/dataset/deblurred/\"\n",
    "denoised_dataset_path = \"./Dataset/1/dataset/denoised/\"\n",
    "\n",
    "classification_dataset_path = \"./Dataset/2/dataset/\"\n",
    "classification_model_path = \"./Models/classification/\"\n",
    "classification_input_path = \"./Dataset/2/input\"\n",
    "classification_output_path = \"./Dataset/2/output\"\n",
    "\n",
    "captionning_dataset_path  = \"./Dataset/3/dataset/train/\"\n",
    "captionning_annotation_path  = \"./Dataset/3/dataset/annotation/\"\n",
    "captionning_model_path = \"./Models/captionning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthodes utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des images\n",
    "def get_image(path, filename):\n",
    "    return io.imread(path + filename)\n",
    "\n",
    "# Sauvegarde des images\n",
    "def save_image(path, filename, content):\n",
    "    #Check if folder exists\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    imageio.imwrite(path + filename , content)\n",
    "\n",
    "# Retrieve importants informations from data    \n",
    "def get_metric_stat(pre_data, post_data):\n",
    "    data = [\n",
    "        np.array([min(pre_data), max(pre_data), np.median(pre_data), np.average(pre_data)]),\n",
    "        np.array([min(post_data), max(post_data), np.median(post_data), np.average(post_data)])\n",
    "    ]\n",
    "\n",
    "    data_array = pd.DataFrame(data,\n",
    "                              index = [\"pre_processed\", \"post_processed\"],\n",
    "                              columns = [\"Min value\", \"Max value\", \"Median value\", \"Average value\"])\n",
    "    print(data_array)\n",
    "\n",
    "# Display all data in table\n",
    "def get_list_data(data_tmp):\n",
    "    data_array = pd.DataFrame(np.array(data_tmp),\n",
    "                              columns = [\"Name\", \"Before\", \"After\"])\n",
    "    print(\"\\n\")\n",
    "    print(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défloutage de l'image\n",
    "\n",
    "Pour le défloutage les images, on passe par le filtrage via **convolution**. L'opération de convolution consiste à faire glisser une autre matrice nommée filtre (de taille généralement inférieure à l'image traitée) tout le long de l'image et remplacer la valeur de chaque pixel de l'image par la somme du produit des éléments de cette matrice.\n",
    "\n",
    "Les filtres d'amélioration de la netteté d'une image (ou filtres d'affutage de contours) permettent d'améliorer la qualité d'une image en accentuant les bords (ou en d'autres termes en accentuant les différences entres les pixels adjacents). L'affutage de contour consiste à prendre des différences.\n",
    "\n",
    "Pour le défloutage des images, on utilise un **filtre Laplacien**.\n",
    "Ce filtre nous permet d'affuter les images grâce à une fonction de convolution de la librairie opencv sur l'image récupérée.\n",
    "La variante de filtre choisie nous permet sur le jeu de données fourni d'affuter les images suffisamment pour retirer le flou présent sans pour autant y ajouter de bruit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication de la matrice Laplacienne\n",
    "\n",
    "\n",
    "L'approximation utilisée pour calculer le Laplacien est (si on prend la somme des l'approximation de la dérivée au sens des abscisses et des ordonnées) exprimée sous la forme $F(x+1,y)+F(x-1,y)+F(x,y+1)+F(x,y-1)-4F(x,y)$).\n",
    "\n",
    "Ce qui nous donne la matrice suivante et sa variante prenant en compte les diagonales (en effet, il existe une multitude de variantes):\n",
    "\n",
    "<img src=\"imageSrc/conv-laplacian.jpg\"/>\n",
    "\n",
    "La matrice de Laplace permet de mettre en évidence les contours d'une image comme on peut le voir selon l'image suivante:\n",
    "\n",
    "<img src=\"imageSrc/laplacian_filtered_image.jpg\"/>\n",
    "\n",
    "On voit donc mieux les contours, mais on perds le sens de l'image de départ. Pour corriger cela, on ajoute donc la matrice identitaire (l'image de départ) sur l'image d'arrivé, d'où le coefficient 9 au lieux de 8 (cf image ci dessous):\n",
    "\n",
    "<img src=\"imageSrc/laplace_conv_original.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblurring function\n",
    "def remove_blur(img, high):\n",
    "    kernel = []\n",
    "    \n",
    "    if high:\n",
    "        # Creation of a Laplacian kernel to use for debluring\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    else:\n",
    "        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    \n",
    "    # Convolution of the kernel with the image given in the function's parameter\n",
    "    return cv2.filter2D(img, -1, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation du niveau de flou\n",
    "\n",
    "Pour évaluer le niveau de flou, on utilise les contours Laplacien sur l'image (cf image ci dessus) et on évalue la variance de Laplace. Une variance faible indique qu'une faible plage de gris est utilisé (en d'autres termes, qu'il n'y a pas beaucoup de nuance utilisé, et donc qui se caractérise par du flou). Une variance élevé correspond donc à une large plage de gris utilisé (donc beaucoup plus de nuance disponible pour les détails). \n",
    "\n",
    "Il faut être vigilant à certaines images, tel qu'une photo du ciel, ou la variance sera faible même si cette dernière est net. L'indicateur devra alors être interprété en fonction du contexte.\n",
    "\n",
    "<img src=\"imageSrc/Laplace_Variance.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blurry_indicator(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(blurry_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "<ul>\n",
    "    <li> Utilisation de threads pour optimiser le temps d'éxecution </li>\n",
    "    <li> Calcul de la variance avant et après traitement </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = blurry_dataset_path + name\n",
    "    img = get_image(blurry_dataset_path,name)\n",
    "\n",
    "     # Get initial Blur metric\n",
    "    original_blur_metric = get_blurry_indicator(img)\n",
    "    pre_processed_data.append(original_blur_metric)\n",
    "    \n",
    "    # Remove blur from the colored image image\n",
    "    deblurred_img = remove_blur(img, high=True)\n",
    "\n",
    "    # Get initial Blur metric\n",
    "    processed_blur_metric = get_blurry_indicator(deblurred_img)\n",
    "    post_processed_data.append(processed_blur_metric)\n",
    "    \n",
    "    #print(\"image \" + name + \" - initial : \" + str(original_blur_metric)\n",
    "     #   + \" - processed : \" + str(processed_blur_metric)\n",
    "     #   + \" - difference : \" + str(processed_blur_metric - original_blur_metric)+\"\\n\")\n",
    "\n",
    "    data_preview_blurr.append([name, original_blur_metric, processed_blur_metric])\n",
    "\n",
    "    # Saving Image\n",
    "    save_image(deblured_dataset_path, name, deblurred_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_blurr = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_blurr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "def display_image_diff(originalPath, diffPath, filename=None):\n",
    "    if not filename:\n",
    "        # Get a random file from directory\n",
    "        filename = random.choice(os.listdir(originalPath)) \n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(get_image(originalPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    # Corrected Image noise\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(get_image(diffPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Corrected Image\")\n",
    "    \n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(blurry_dataset_path, deblured_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débruitage\n",
    "La capture d'un signal lumineux par un appareil photographique s'accompagne le plus souvent d'informations non désirées : le « bruit ». \n",
    "L'essentiel de ce « bruit » (des pixels trop clairs ou trop sombre en trop grand nombre ou de manière irrégulière, par exemple) est dû au capteur.\n",
    "\n",
    "### Le débruitage par morceaux (par patchs)\n",
    "Le débruitage par morceaux est une technique de débruitage d'image utilisant l'algorithme de réduction du bruit numérique appelé en Anglais \"non-local means\".\n",
    "La méthode repose sur un principe simple, remplacer la couleur d'un pixel par une moyenne des couleurs de pixels similaires. Mais les pixels les plus similaires à un pixel donné n'ont aucune raison d'être proches. Il est donc nécessaire de scanner une vaste partie de l'image à la recherche de tous les pixels qui ressemblent vraiment au pixel que l'on veut débruiter.\n",
    "\n",
    "### Pourquoi cette methode ?\n",
    "Le résultat d'un tel filtrage permet d’amoindrir la perte de détails au sein de l'image, comparé aux filtres réalisant des moyennes localement tel que le filtre de Gauss ou le filtre de Wiener, le bruit généré par l'algorithme \"non-local means\" est plus proche du bruit blanc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax:**\n",
    "cv2.fastNlMeansDenoisingColored( P1, P2, float P3, float P4, int P5, int P6)\n",
    "\n",
    "**Parameters:**\n",
    "* P1 – Source Image Array\n",
    "* P2 – Destination Image Array\n",
    "* P3 – Size in pixels of the template patch that is used to compute weights.\n",
    "* P4 – Size in pixels of the window that is used to compute a weighted average for the given pixel.\n",
    "* P5 – Parameter regulating filter strength for luminance component.\n",
    "* P6 – Same as above but for color components // Not used in a grayscale image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Noise function\n",
    "def remove_noise(image, high):\n",
    "    if high == 2:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "    elif high == 1:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 5, 10, 7, 15)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 15)\n",
    "\n",
    "def estimate_noise(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return estimate_sigma(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(noisy_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = noisy_dataset_path + name\n",
    "    img = get_image(noisy_dataset_path,name)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    original_noise_metric = estimate_noise(img)\n",
    "    pre_processed_data.append(original_noise_metric)\n",
    "    \n",
    "    denoised_img = remove_noise(img, high=2)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    processed_noise_metric = estimate_noise(denoised_img)\n",
    "    post_processed_data.append(processed_noise_metric)\n",
    "\n",
    "    data_preview_denoised.append([name, original_noise_metric, processed_noise_metric])\n",
    "    \n",
    "    save_image(denoised_dataset_path, name, denoised_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_denoised = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(noisy_dataset_path, denoised_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation entre Défloutage et Débruitage\n",
    "Afin d'améliorer l'image au maximum, on peut effectuer les 2 operations à savoir, le traitement de bruit et le traitement de flou, sur une même image et utiliser nos métriques de performances pour automatiser l'application des traitements. Un autre point d'importance, l'ordre dans lequel on effectue les traitements a un impact sur la qualité de l'image.\n",
    "\n",
    "### Procédé d'amelioration\n",
    "Afin de determiner la qualité générale d'une image en termes de bruit et de flou, on utilise la moyenne des mesures effectuées précedement sur les differents jeux de tests. En fonction de la première mesure de l'image, on décide de commencer par un débruitage ou un défloutage. Ensuite, une nouvelle mesure est effectuée et est comparée à nouveau avec les mesures des jeux de données post traitement. \n",
    "Si l'image n'est toujours pas considérées viable, on effectue l'opération inverse à plus faible intensité pour essayer d'avoir le meilleur compromis. Pour ces traitements, on se limite à deux passages pour éviter de détériorer l'image à analyser et pour éviter des temps de traitements trop longs. \n",
    "\n",
    "### Résultats\n",
    "Le résultat de ces tests a permis de demontrer que la meilleure solution consiste a commencer par effectuer un defloutage à forte intensité sur l'image puis, si c'est nécessaire, un debruitage à basse intensité. Cela représente le meilleur compromis au niveau de la qualité génerale de l'image.\n",
    "\n",
    "L'affichage de ces test est disponible ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose noisy or blurry image\n",
    "noisy = False\n",
    "\n",
    "img = []\n",
    "#if you want random testing\n",
    "#img = get_image(\"./Dataset/Blurry/\", random.choice(os.listdir(\"./Dataset/Blurry/\")))\n",
    "\n",
    "#image retrival\n",
    "if noisy:\n",
    "    img = get_image(noisy_dataset_path, \"noisy_117.jpg\")\n",
    "\n",
    "else:\n",
    "    img = get_image(blurry_dataset_path, \"blurry_092.jpg\")\n",
    "\n",
    "#initial image measurements\n",
    "initial_noise = estimate_noise(img)\n",
    "initial_blur = get_blurry_indicator(img)\n",
    "\n",
    "# print(initial_noise, initial_blur)\n",
    "\n",
    "#image is blurry\n",
    "if initial_blur < 3000:\n",
    "    # high deblur of the image\n",
    "    img_stage2 = remove_blur(img, high=False)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of noise\n",
    "    if second_noise < 1:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image doesn't meets requirements in terms of noise\n",
    "    else:\n",
    "        #low denoise of the image\n",
    "        img_stage3 = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        img_stage3_low_noise = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "#if image is noisy      \n",
    "if initial_noise > 1:\n",
    "    # high denoise of the image\n",
    "    img_stage2 = remove_noise(img, 2)\n",
    "    \n",
    "    img_stage2_low_noise = remove_noise(img, 1)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    if second_blur > 48000:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(123)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    else:\n",
    "        #low deblur of the image\n",
    "        img_stage3 = remove_blur(img_stage2, high=False)\n",
    "        img_stage3_low_noise = remove_blur(img_stage2, high=False)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 denoise\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 medium denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 denoise and 1 low deblur\")\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 medium denoise and 1 low deblur\")\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "## Image\n",
    "\n",
    "\n",
    "## Défloutage\n",
    "\n",
    "* https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "* https://stackoverflow.com/questions/48319918/whats-the-theory-behind-computing-variance-of-an-image\n",
    "\n",
    "## Debruitage\n",
    "* https://docs.opencv.org/3.4/d5/d69/tutorial_py_non_local_means.html\n",
    "* http://www.ipol.im/pub/art/2011/bcm_nlm/article.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2 - Classification binaire\n",
    "\n",
    "L’entreprise voulant automatiser la sélection de photos pour l’annotations, le livrable 2 devra fournir une méthode de classification se basant sur les réseaux de neurones afin de filtrer les images qui ne sont pas des photos du dataset de départ.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code TensorFlow ainsi qu’un schéma de l’architecture du réseau de neurones. Toutes les parties doivent être détaillée dans le notebook : les paramètre du réseau, la fonction de perte ainsi que l’algorithme d’optimisation utilisé pour l’entrainement.</li>\n",
    "    <li>Un graphique contenant l’évolution de l’erreur d’entrainement ainsi que de l’erreur de test et l’évolution de l’accuracy pour ces deux datasets.</li>\n",
    "    <li>L’analyse de ces résultats, notamment le compromis entre biais et variance (ou sur-apprentissage et sous-apprentissage).</li>\n",
    "    <li>Une description des méthodes potentiellement utilisables pour améliorer les compromis biais/variance : technique de régularisation, drop out, early-stopping, …</li>\n",
    "</ul>\n",
    "\n",
    "Le but ultime est d’être capable de distinguer les photos parmi toutes ces images. Il est tout de même conseillé de commencer par les images les plus faciles à distinguer des photos, puis aller vers les dataset les plus difficiles à classifier (notamment, il y a dans le dataset peinture un certain nombre d’oeuvres au rendu assez réaliste, qui devraient vous poser problème).\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/01/2021</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics checks for image classifications\n",
    "# print(\"executing tensorflow version \" + tf.__version__)\n",
    "\n",
    "if (len(tf.config.experimental.list_physical_devices('GPU')) == 1):\n",
    "    print(\"GPU is detected\")\n",
    "else :\n",
    "    print(\"GPU isn't detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramétres\n",
    "\n",
    "**Batch Size** : \n",
    "\n",
    "le nombre d'échantillons qui seront propagés à travers le réseau a chaque itération.\n",
    "\n",
    "Le Batch size utilisé classiquement est de 32 mais nous l'avons reduit a 16, il a été empiriquement démontré que l'utilisation de lots plus petits permettait une convergence plus rapide vers de \"bonnes\" solutions. Cela s'explique notament par le fait que des tailles de lot plus petites permettent au modèle de commencer à apprendre avant de voir toutes les données.\n",
    "\n",
    "**La taille de l'image** :\n",
    "\n",
    "Reduit a 200*200 dans un soucis de preservation de la memoire vive.\n",
    "\n",
    "**Validation Split** : \n",
    "\n",
    "Il s'agit du paramètre qui spécifie la taille des données de formation qui seront utilisées pour la validation. C'est une valeur flottante entre 0 et 1. Les données de validation ne sont pas utilisées pour la formation, mais pour évaluer la perte et la précision.\n",
    "\n",
    "Nous l'avons fixé a 20% car dans de nombreux article et autre exemple, nous avons vu que la valeur etait generalement fixé a ce seuil.\n",
    "\n",
    "**Epochs** :\n",
    "\n",
    "C'est un hyperparamètre qui définit le nombre de fois que l'algorithme d'apprentissage fonctionnera sur l'ensemble des données de formation.\n",
    "\n",
    "Dans notre test final, nous avons mis 20 epochs car la validation accuracy n'augmente plus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the dataset (amount of images per batch, image resolution and training percentage)\n",
    "batch_size = 16\n",
    "img_height = 200\n",
    "img_width = 200\n",
    "validation_split = 0.2\n",
    "epochs=20\n",
    "classes = ['Photo', 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Modèle\n",
    "\n",
    "<img src=\"imageSrc\\ShemaNeural.PNG\">\n",
    "\n",
    "Le modèle se compose de trois blocs de convolution avec une couche de pool maximum dans chacun d'eux.\n",
    "\n",
    "Il y a une couche entièrement connectée avec 128 unités sur le dessus qui est activée par une fonction d'activation \"relu\". \n",
    "\n",
    "Ce modèle basique trouvé sur tensorflow tutoriel a eté modifié via les parametres et l'ajout de layers avec nos nombreux tests.\n",
    "\n",
    "**Le kernel** : \n",
    "\n",
    "Nous avons eu le choix entre plusieurs kernel 1x1, 2x2, 3x3, 7x7, 9x9, etc...  \n",
    "L'une des raisons pour lesquelles nous avons préfère les noyaux de petite taille aux réseaux entièrement connectés est qu'ils réduisent les coûts de calcul et le partage des      poids, ce qui conduit en fin de compte à des poids moins importants.\n",
    "\n",
    "1x1 a été éliminé car les caractéristiques extraites seront finement granulées et locales donnant aucune information provenant des pixels voisins.\n",
    "\n",
    "Et nous avons eliminé tout les kernel de nombre pair car il n'y a pas de valeur central et cela creer une perte de precision.\n",
    "\n",
    "**Relu**:\n",
    "\n",
    " \"Rectified Linear Unit\" est une fonction d'activation linéaire par morceaux, c'est la methode la plus utilisé de nos jours. Son avantage réside sur le fait qu'elle remplace toute valeur d'entrée négative par 0 et toute valeur positive par elle même.\n",
    "\n",
    "Nous avons choisi de rajouter une convolution au model de base dans un besoins de précision, cela a amelioré nos resultats.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def generate_model():\n",
    "    #generation of the training dataset\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    #retrieve the amount of classes for the model\n",
    "    num_classes = len(train_ds.class_names)\n",
    "    print(\"Classes found : \" + str(num_classes))\n",
    "    print(train_ds.class_names)\n",
    "\n",
    "    #Allow for perfomance compilation times by preventing IO bottleneck on disks while compiling the model\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #Structure of the neural network\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.Conv2D(16, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(64, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dropout(0.2),\n",
    "      layers.Dense(2)\n",
    "    ])\n",
    "    \n",
    "    #display neural network structure\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy', get_f1])\n",
    "    \n",
    "    #amount of training and fitting\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "    \n",
    "    #display statitics over training accuracy\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def classify_image(model, classes, impath):\n",
    "    #load disj image\n",
    "    img = image.load_img((impath), target_size=(img_height, img_width))\n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "\n",
    "    #use model to predict classe\n",
    "    prediction = model.predict(img)\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    #     print(\n",
    "    #         \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    #         .format(classes[np.argmax(score)], 100 * np.max(score))\n",
    "    #     )\n",
    "    #return clas and percentage of confidence\n",
    "    return [classes[np.argmax(score)], 100 * np.max(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(newModel=False):\n",
    "    # Checking if the Models folder is empty\n",
    "    if newModel == True: \n",
    "        model = generate_model()\n",
    "        tf.keras.models.save_model(model, classification_model_path)\n",
    "        return model    \n",
    "    # Or not\n",
    "    else:\n",
    "        return tf.keras.models.load_model(classification_model_path)\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuil de confiance à partir duquel les images 'Photo' sont acceptées\n",
    "confidence_threshold = 90\n",
    "\n",
    "# Classify images and moved \"Photo\" to the output folder \n",
    "for pictures in os.listdir(classification_input_path):\n",
    "    res_classe, res_score = classify_image(model, classes, os.path.join(classification_input_path,pictures))\n",
    "    print(pictures, res_classe, res_score, '%')\n",
    "\n",
    "    if (res_classe == 'Photo' and res_score > confidence_threshold):\n",
    "        picture_path = classification_input_path + \"/\" + pictures\n",
    "        shutil.copy2(picture_path, classification_output_path) \n",
    "\n",
    "# Single image test\n",
    "#img_name = 'photo_0001.jpg'\n",
    "#result = classify_image(model, classes, './Dataset/2/input/'+img_name)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultat\n",
    "\n",
    "Dans un premier temps nous avons créer un modele qui prend en input les 5 classes et rend 5 classes en sortie. Ce modele permet donc d'identifier le type d'image en entrée. \n",
    "\n",
    "\n",
    "<img src=\"imageSrc\\resultLiv2.png\">\n",
    "\n",
    "**Premier test**\n",
    "\n",
    "Premier résultat : \n",
    "\n",
    "Voici les résultats obtenus apres la compilation du modele avec les hyperparametres suivant :\n",
    "\n",
    "Epochs : 20\n",
    "\n",
    "Resolution de l'image : 200*200\n",
    "\n",
    "Bach size : 16\n",
    "\n",
    "Kernel : 3*3\n",
    "\n",
    "Notre modele n'est pas optimal :\n",
    "- Le training and Validation accuracy nous indique des erreurs de validation, une faible précision de validation par rapport à la précision de formation, ce qui indique un fort overfitting. \n",
    "\n",
    "- Le training and Validation loss quant a lui nous montre que notre modele a une augmentation des erreurs sur la validation notre modele a besoins d'etre modifié.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imageSrc\\2resultLiv2.png\">\n",
    "\n",
    "**Second test**\n",
    "\n",
    "Epochs : 20\n",
    "\n",
    "Resolution de l'image : 200*200\n",
    "\n",
    "Bach size : 16\n",
    "\n",
    "Kernel : 3*3\n",
    "\n",
    "Droupout : 50% \n",
    "\n",
    "**Dropout** est une technique où des neurones choisis au hasard sont ignorés pendant l'entraînement. Ils sont \"abandonnés\" au hasard. Cela signifie que leur contribution à l'activation des neurones en aval est temporairement supprimée lors du passage et toute mise à jour du poids n'est pas appliquée au neurone.\n",
    "\n",
    "Notre modele n'est pas optimal :\n",
    "- Le training and Validation accuracy : la courbe ressemble un peu plus a ce que l'on veut voir mais la precision a baissé.\n",
    "\n",
    "- Le training and Validation loss quand a lui nous montre que notre modele a un bon taux d'apprentissage et la validation loss est decendant.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imageSrc\\3graphliv2.png\">\n",
    "\n",
    "**Troisieme Test**\n",
    "\n",
    "Nous avons changé de modele pour un modele binaire, nous prenons en entré seulement les photos et autres type d'image. En sorti, nous avons l'identification des images en tant que photo ou non.\n",
    "Nous avons reduit aussi les epochs pour eviter le surapprentissage du reseau de neurone.\n",
    "\n",
    "Epochs : 8\n",
    "\n",
    "Resolution de l'image : 200*200\n",
    "\n",
    "Bach size : 16\n",
    "\n",
    "Kernel : 3*3\n",
    "\n",
    "Droupout : 50% \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imageSrc\\4resultliv2.png\">\n",
    "\n",
    "**Quatrieme Test réussite**\n",
    "\n",
    "Nous avons changé de modèle pour un modèle binaire, nous prenons en entrée seulement les photos et autres types d'image. En sortit, nous avons l'identification des images en tant que photo ou non.\n",
    "Nous avons réduit aussi les epochs pour éviter le surapprentissage du réseau de neurones.\n",
    "\n",
    "Epochs : 20\n",
    "\n",
    "Resolution de l'image : 200*200\n",
    "\n",
    "Bach size : 16, 32, 64\n",
    "\n",
    "Kernel : 3*3\n",
    "\n",
    "Droupout : 5% \n",
    "\n",
    "Nous avons un résultat très satisfaisant montrant une précision sur la validation suivant bien le training accuracy et qui atteint un degret de précision haut, la perte sur la validation baisse aussi significativement en parallèle de la perte sur le training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test autre model\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ])\n",
    "    \n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape= (img_height,img_width) + (3,), num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}