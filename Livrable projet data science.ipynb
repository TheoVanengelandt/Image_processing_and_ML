{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Livrable Projet DATA SCIENCE</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "L'entreprise TouNum est une entreprise de numérisation de documents. Elle prospose différents services dont la numérisation de base de document papier pour les entreprises clientes. TouNum veut optimiser et rendre intelligent ce processus de scanning en incluant des outils de Machine Learning. Le gain de temps serait important aux vues des nombreuses données que l'entreprise doit scanner et étiqueter.\n",
    "Pour cela, TouNum fait appel à CESI pour réaliser cette prestation.\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est que l'équipe de data scientist de CESI réalise cette solution visant à analyser des photographies pour en déterminer une légende descriptive de manière automatique. Il faudra également améliorer la qualité des images scannées ayant des qualités variables (parfois floues, ou bruitées).\n",
    "\n",
    "<img src=\"imageSrc/caption image.PNG\"/>\n",
    "\n",
    "### Enjeux\n",
    "\n",
    "TouNum devait trier et étiqueter chaque document scanné. La solution délivré par CESI permet l'automatisation de ces tâches en faisant donc gagner un temps non négligeable. Elle va donc pouvoir réaliser plus de contrats et augmenter la satisfaction client.\n",
    "\n",
    "### Contraintes techniques\n",
    "\n",
    "L'implémentation des algorithmes doit être réaliser sur Python, notamment les librairies Scikit et TensorFlow. La librairie Pandas doit être utilisé pour manipuler le dataset et ImageIO pour le charger. NumPy et MatPlotLib seront nécessaire pour le calcul scientifique et la modélisation.\n",
    "\n",
    "Le programme à livrer devra respecter le workflow suivant :\n",
    "\n",
    "<img src=\"imageSrc/workflow.PNG\"/>\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "La classification d'image se fera à l'aide de réseaux de neurones. Cette dernière doit distinguer les photos d'un autre documents, tel que schémas, textes scannés, voir peintures.\n",
    "TouNoum possède un dataset rempli d'images divers pour entrainer le réseau de neurones.\n",
    "\n",
    "#### Prétraitement\n",
    "\n",
    "Le prétraitement dois utiliser des filtres convolutifs afin d'améliorer la qualité. Il doit établir un compromis entre débruitage et affutage.\n",
    "\n",
    "#### Captionning\n",
    "\n",
    "Le Captionning devra légender automatiquement les images. Il utilisera deux techniques de Machine Learning : les réseaux de neurones convolutifs (CNN) pour prétraiter l'image en identifiant les zones d’intérêt, et les réseaux de neurones récurrents (RNN) pour générer les étiquettes. Il faudra être vigilant quant aux ressources RAM. Un dataset d'étiquetage classique est disponible pour l’apprentissage supervisé.\n",
    "\n",
    "### Livrable\n",
    "\n",
    "La solution doit sous forme de notebook Jupiter entièrement automatisé. Il doit être conçu pour être faciliter mis en production et maintenance.\n",
    "Il faut démontrer la pertinence du modèle de manière rigoureuse et pédagogique.\n",
    "\n",
    "#### Jalons\n",
    "\n",
    "CESI devra dois rendre le prototype complet et fonctionnel du programme pour le 23 janvier. \n",
    "TouNum exige également 3 dates de rendu pour suivre la bonne avancé du projet.\n",
    "<ul>\n",
    "    <li>18/12/20 : Prétraitement d'image</li>\n",
    "    <li>15/01/21 : Classification binaire</li>\n",
    "    <li>20/01/21 : Captioning d'images</li>\n",
    "    <li>22/01/21 : Démonstration </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Importation des librairies utilisées*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Check if imageio package is installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio\n",
    "    \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if cikit-image package is installed\n",
    "try:\n",
    "    import skimage\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "from skimage import io\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "# Check if opencv-python package is installed\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    !pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Check if pandas package is installed\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "# Check if tensorflow package is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import tensorflow_datasets\n",
    "except ImportError:\n",
    "    !pip install tensorflow_datasets    \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    !pip install keras    \n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Livrable 3 \n",
    "import json\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Check if tqdm package is installed\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    !pip install -q tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Chemins physiques*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_dataset_path = \"./Dataset/1/dataset/Blurry/\"\n",
    "noisy_dataset_path = \"./Dataset/1/dataset/Noisy/\"\n",
    "deblured_dataset_path = \"./Dataset/1/dataset/deblurred/\"\n",
    "denoised_dataset_path = \"./Dataset/1/dataset/denoised/\"\n",
    "\n",
    "classification_dataset_path = \"./Dataset/2/dataset/\",\n",
    "classification_model_path = \"./Models/classification/\",\n",
    "\n",
    "captionning_dataset_path  = \"./Dataset/3/dataset/train/\",\n",
    "captionning_annotation_path  = \"./Dataset/3/dataset/annotation/\",\n",
    "captionning_model_path = \"./Models/captionning/\"\n",
    "\n",
    "captioning_input_path = './Dataset/3/input'\n",
    "captioning_output_path = './Dataset/3/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthodes utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des images\n",
    "def get_image(path, filename):\n",
    "    return io.imread(path + filename)\n",
    "\n",
    "# Sauvegarde des images\n",
    "def save_image(path, filename, content):\n",
    "    #Check if folder exists\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    imageio.imwrite(path + filename , content)\n",
    "\n",
    "# Retrieve importants informations from data    \n",
    "def get_metric_stat(pre_data, post_data):\n",
    "    data = [\n",
    "        np.array([min(pre_data), max(pre_data), np.median(pre_data), np.average(pre_data)]),\n",
    "        np.array([min(post_data), max(post_data), np.median(post_data), np.average(post_data)])\n",
    "    ]\n",
    "\n",
    "    data_array = pd.DataFrame(data,\n",
    "                              index = [\"pre_processed\", \"post_processed\"],\n",
    "                              columns = [\"Min value\", \"Max value\", \"Median value\", \"Average value\"])\n",
    "    print(data_array)\n",
    "\n",
    "# Display all data in table\n",
    "def get_list_data(data_tmp):\n",
    "    data_array = pd.DataFrame(np.array(data_tmp),\n",
    "                              columns = [\"Name\", \"Before\", \"After\"])\n",
    "    print(\"\\n\")\n",
    "    print(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable 1 - Prétraitement (denoising/sharpening…)\n",
    "\n",
    "Le but est de traiter un ensemble de photographies afin de les rendre mieux traitables par les algorithmes de Machine Learning. Il y a deux traitements à réaliser : le débruitage, et l’affutage. Vous devrez produire un notebook Jupyter explicitant ces étapes de prétraitement, et leurs performances. Ces algorithmes s’appuieront sur des notions assez simples autour des filtres de convolution, et les appliqueront pour améliorer la qualité de l’image. Il faudra notamment décider d’un compromis entre dé-bruitage et affutage.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code de chargement du fichier.</li>\n",
    "    <li>Le code du débruitage sur un sous-ensemble d’images bruitées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>Le code de l’affutage sur un sous-ensembles d’images floutées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>\n",
    "        Une étude de cas explicitant les compromis entre ces deux opérations. Cette partie du livrable doit inclure le bruitage d’images et montrer la perte de détails, ou l’affutage d’images et montrer l’apparition du bruit.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/12/2020</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défloutage de l'image\n",
    "\n",
    "Pour le défloutage les images, on passe par le filtrage via **convolution**. L'opération de convolution consiste à faire glisser une autre matrice nommée filtre (de taille généralement inférieure à l'image traitée) tout le long de l'image et remplacer la valeur de chaque pixel de l'image par la somme du produit des éléments de cette matrice.\n",
    "\n",
    "Les filtres d'amélioration de la netteté d'une image (ou filtres d'affutage de contours) permettent d'améliorer la qualité d'une image en accentuant les bords (ou en d'autres termes en accentuant les différences entres les pixels adjacents). L'affutage de contour consiste à prendre des différences.\n",
    "\n",
    "Pour le défloutage des images, on utilise un **filtre Laplacien**.\n",
    "Ce filtre nous permet d'affuter les images grâce à une fonction de convolution de la librairie opencv sur l'image récupérée.\n",
    "La variante de filtre choisie nous permet sur le jeu de données fourni d'affuter les images suffisamment pour retirer le flou présent sans pour autant y ajouter de bruit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication de la matrice Laplacienne\n",
    "\n",
    "\n",
    "L'approximation utilisée pour calculer le Laplacien est (si on prend la somme des l'approximation de la dérivée au sens des abscisses et des ordonnées) exprimée sous la forme $F(x+1,y)+F(x-1,y)+F(x,y+1)+F(x,y-1)-4F(x,y)$).\n",
    "\n",
    "Ce qui nous donne la matrice suivante et sa variante prenant en compte les diagonales (en effet, il existe une multitude de variantes):\n",
    "\n",
    "<img src=\"imageSrc/conv-laplacian.jpg\"/>\n",
    "\n",
    "La matrice de Laplace permet de mettre en évidence les contours d'une image comme on peut le voir selon l'image suivante:\n",
    "\n",
    "<img src=\"imageSrc/laplacian_filtered_image.jpg\"/>\n",
    "\n",
    "On voit donc mieux les contours, mais on perds le sens de l'image de départ. Pour corriger cela, on ajoute donc la matrice identitaire (l'image de départ) sur l'image d'arrivé, d'où le coefficient 9 au lieux de 8 (cf image ci dessous):\n",
    "\n",
    "<img src=\"imageSrc/laplace_conv_original.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblurring function\n",
    "def remove_blur(img, high):\n",
    "    kernel = []\n",
    "    \n",
    "    if high:\n",
    "        # Creation of a Laplacian kernel to use for debluring\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    else:\n",
    "        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    \n",
    "    # Convolution of the kernel with the image given in the function's parameter\n",
    "    return cv2.filter2D(img, -1, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation du niveau de flou\n",
    "\n",
    "Pour évaluer le niveau de flou, on utilise les contours Laplacien sur l'image (cf image ci dessus) et on évalue la variance de Laplace. Une variance faible indique qu'une faible plage de gris est utilisé (en d'autres termes, qu'il n'y a pas beaucoup de nuance utilisé, et donc qui se caractérise par du flou). Une variance élevé correspond donc à une large plage de gris utilisé (donc beaucoup plus de nuance disponible pour les détails). \n",
    "\n",
    "Il faut être vigilant à certaines images, tel qu'une photo du ciel, ou la variance sera faible même si cette dernière est net. L'indicateur devra alors être interprété en fonction du contexte.\n",
    "\n",
    "<img src=\"imageSrc/Laplace_Variance.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blurry_indicator(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(blurry_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "<ul>\n",
    "    <li> Utilisation de threads pour optimiser le temps d'éxecution </li>\n",
    "    <li> Calcul de la variance avant et après traitement </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = blurry_dataset_path + name\n",
    "    img = get_image(blurry_dataset_path,name)\n",
    "\n",
    "     # Get initial Blur metric\n",
    "    original_blur_metric = get_blurry_indicator(img)\n",
    "    pre_processed_data.append(original_blur_metric)\n",
    "    \n",
    "    # Remove blur from the colored image image\n",
    "    deblurred_img = remove_blur(img, high=True)\n",
    "\n",
    "    # Get initial Blur metric\n",
    "    processed_blur_metric = get_blurry_indicator(deblurred_img)\n",
    "    post_processed_data.append(processed_blur_metric)\n",
    "    \n",
    "    #print(\"image \" + name + \" - initial : \" + str(original_blur_metric)\n",
    "     #   + \" - processed : \" + str(processed_blur_metric)\n",
    "     #   + \" - difference : \" + str(processed_blur_metric - original_blur_metric)+\"\\n\")\n",
    "\n",
    "    data_preview_blurr.append([name, original_blur_metric, processed_blur_metric])\n",
    "\n",
    "    # Saving Image\n",
    "    save_image(deblured_dataset_path, name, deblurred_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_blurr = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_blurr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "def display_image_diff(originalPath, diffPath, filename=None):\n",
    "    if not filename:\n",
    "        # Get a random file from directory\n",
    "        filename = random.choice(os.listdir(originalPath)) \n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(get_image(originalPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    # Corrected Image noise\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(get_image(diffPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Corrected Image\")\n",
    "    \n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(blurry_dataset_path, deblured_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débruitage\n",
    "La capture d'un signal lumineux par un appareil photographique s'accompagne le plus souvent d'informations non désirées : le « bruit ». \n",
    "L'essentiel de ce « bruit » (des pixels trop clairs ou trop sombre en trop grand nombre ou de manière irrégulière, par exemple) est dû au capteur.\n",
    "\n",
    "### Le débruitage par morceaux (par patchs)\n",
    "Le débruitage par morceaux est une technique de débruitage d'image utilisant l'algorithme de réduction du bruit numérique appelé en Anglais \"non-local means\".\n",
    "La méthode repose sur un principe simple, remplacer la couleur d'un pixel par une moyenne des couleurs de pixels similaires. Mais les pixels les plus similaires à un pixel donné n'ont aucune raison d'être proches. Il est donc nécessaire de scanner une vaste partie de l'image à la recherche de tous les pixels qui ressemblent vraiment au pixel que l'on veut débruiter.\n",
    "\n",
    "### Pourquoi cette methode ?\n",
    "Le résultat d'un tel filtrage permet d’amoindrir la perte de détails au sein de l'image, comparé aux filtres réalisant des moyennes localement tel que le filtre de Gauss ou le filtre de Wiener, le bruit généré par l'algorithme \"non-local means\" est plus proche du bruit blanc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax:**\n",
    "cv2.fastNlMeansDenoisingColored( P1, P2, float P3, float P4, int P5, int P6)\n",
    "\n",
    "**Parameters:**\n",
    "* P1 – Source Image Array\n",
    "* P2 – Destination Image Array\n",
    "* P3 – Size in pixels of the template patch that is used to compute weights.\n",
    "* P4 – Size in pixels of the window that is used to compute a weighted average for the given pixel.\n",
    "* P5 – Parameter regulating filter strength for luminance component.\n",
    "* P6 – Same as above but for color components // Not used in a grayscale image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Noise function\n",
    "def remove_noise(image, high):\n",
    "    if high == 2:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "    elif high == 1:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 5, 10, 7, 15)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 15)\n",
    "\n",
    "def estimate_noise(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return estimate_sigma(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(noisy_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = noisy_dataset_path + name\n",
    "    img = get_image(noisy_dataset_path,name)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    original_noise_metric = estimate_noise(img)\n",
    "    pre_processed_data.append(original_noise_metric)\n",
    "    \n",
    "    denoised_img = remove_noise(img, high=2)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    processed_noise_metric = estimate_noise(denoised_img)\n",
    "    post_processed_data.append(processed_noise_metric)\n",
    "\n",
    "    data_preview_denoised.append([name, original_noise_metric, processed_noise_metric])\n",
    "    \n",
    "    save_image(denoised_dataset_path, name, denoised_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_denoised = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(noisy_dataset_path, denoised_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation entre Défloutage et Débruitage\n",
    "Afin d'améliorer l'image au maximum, on peut effectuer les 2 operations à savoir, le traitement de bruit et le traitement de flou, sur une même image et utiliser nos métriques de performances pour automatiser l'application des traitements. Un autre point d'importance, l'ordre dans lequel on effectue les traitements a un impact sur la qualité de l'image.\n",
    "\n",
    "### Procédé d'amelioration\n",
    "Afin de determiner la qualité générale d'une image en termes de bruit et de flou, on utilise la moyenne des mesures effectuées précedement sur les differents jeux de tests. En fonction de la première mesure de l'image, on décide de commencer par un débruitage ou un défloutage. Ensuite, une nouvelle mesure est effectuée et est comparée à nouveau avec les mesures des jeux de données post traitement. \n",
    "Si l'image n'est toujours pas considérées viable, on effectue l'opération inverse à plus faible intensité pour essayer d'avoir le meilleur compromis. Pour ces traitements, on se limite à deux passages pour éviter de détériorer l'image à analyser et pour éviter des temps de traitements trop longs. \n",
    "\n",
    "### Résultats\n",
    "Le résultat de ces tests a permis de demontrer que la meilleure solution consiste a commencer par effectuer un defloutage à forte intensité sur l'image puis, si c'est nécessaire, un debruitage à basse intensité. Cela représente le meilleur compromis au niveau de la qualité génerale de l'image.\n",
    "\n",
    "L'affichage de ces test est disponible ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose noisy or blurry image\n",
    "noisy = False\n",
    "\n",
    "img = []\n",
    "#if you want random testing\n",
    "#img = get_image(\"./Dataset/Blurry/\", random.choice(os.listdir(\"./Dataset/Blurry/\")))\n",
    "\n",
    "#image retrival\n",
    "if noisy:\n",
    "    img = get_image(noisy_dataset_path, \"noisy_117.jpg\")\n",
    "\n",
    "else:\n",
    "    img = get_image(blurry_dataset_path, \"blurry_092.jpg\")\n",
    "\n",
    "#initial image measurements\n",
    "initial_noise = estimate_noise(img)\n",
    "initial_blur = get_blurry_indicator(img)\n",
    "\n",
    "# print(initial_noise, initial_blur)\n",
    "\n",
    "#image is blurry\n",
    "if initial_blur < 3000:\n",
    "    # high deblur of the image\n",
    "    img_stage2 = remove_blur(img, high=False)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of noise\n",
    "    if second_noise < 1:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image doesn't meets requirements in terms of noise\n",
    "    else:\n",
    "        #low denoise of the image\n",
    "        img_stage3 = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        img_stage3_low_noise = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "#if image is noisy      \n",
    "if initial_noise > 1:\n",
    "    # high denoise of the image\n",
    "    img_stage2 = remove_noise(img, 2)\n",
    "    \n",
    "    img_stage2_low_noise = remove_noise(img, 1)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    if second_blur > 48000:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(123)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    else:\n",
    "        #low deblur of the image\n",
    "        img_stage3 = remove_blur(img_stage2, high=False)\n",
    "        img_stage3_low_noise = remove_blur(img_stage2, high=False)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 denoise\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 medium denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 denoise and 1 low deblur\")\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 medium denoise and 1 low deblur\")\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "## Image\n",
    "\n",
    "\n",
    "## Défloutage\n",
    "\n",
    "* https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "* https://stackoverflow.com/questions/48319918/whats-the-theory-behind-computing-variance-of-an-image\n",
    "\n",
    "## Debruitage\n",
    "* https://docs.opencv.org/3.4/d5/d69/tutorial_py_non_local_means.html\n",
    "* http://www.ipol.im/pub/art/2011/bcm_nlm/article.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2 - Classification binaire\n",
    "\n",
    "L’entreprise voulant automatiser la sélection de photos pour l’annotations, le livrable 2 devra fournir une méthode de classification se basant sur les réseaux de neurones afin de filtrer les images qui ne sont pas des photos du dataset de départ.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code TensorFlow ainsi qu’un schéma de l’architecture du réseau de neurones. Toutes les parties doivent être détaillée dans le notebook : les paramètre du réseau, la fonction de perte ainsi que l’algorithme d’optimisation utilisé pour l’entrainement.</li>\n",
    "    <li>Un graphique contenant l’évolution de l’erreur d’entrainement ainsi que de l’erreur de test et l’évolution de l’accuracy pour ces deux datasets.</li>\n",
    "    <li>L’analyse de ces résultats, notamment le compromis entre biais et variance (ou sur-apprentissage et sous-apprentissage).</li>\n",
    "    <li>Une description des méthodes potentiellement utilisables pour améliorer les compromis biais/variance : technique de régularisation, drop out, early-stopping, …</li>\n",
    "</ul>\n",
    "\n",
    "Le but ultime est d’être capable de distinguer les photos parmi toutes ces images. Il est tout de même conseillé de commencer par les images les plus faciles à distinguer des photos, puis aller vers les dataset les plus difficiles à classifier (notamment, il y a dans le dataset peinture un certain nombre d’oeuvres au rendu assez réaliste, qui devraient vous poser problème).\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/01/2021</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics checks for image classifications\n",
    "print(\"executing tensorflow version \" + tf.__version__)\n",
    "if (len(tf.config.experimental.list_physical_devices('GPU')) == 1):\n",
    "    print(\"GPU is detected\")\n",
    "else :\n",
    "    print(\"GPU isn't detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the dataset (amount of images per batch, image resolution and training percentage)\n",
    "batch_size = 32\n",
    "img_height = 250\n",
    "img_width = 250\n",
    "validation_split = 0.3\n",
    "classes = ['Painting', 'Photo', 'Schematics', 'Sketch', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    #generation of the training dataset\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    #retrieve the amount of classes for the model\n",
    "    num_classes = len(train_ds.class_names)\n",
    "    print(\"Classes found : \" + str(num_classes))\n",
    "    print(train_ds.class_names)\n",
    "\n",
    "    #Allow for perfomance compilation times by preventing IO bottleneck on disks while compiling the model\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #Structure of the neural network\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.Conv2D(batch_size, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "    \n",
    "    #displat neural network structure\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "      optimizer='adam',\n",
    "      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    #amount of training and fitting\n",
    "    epochs=30\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "    \n",
    "    #display statitics over training accuracy\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def classify_image(model, classes, impath):\n",
    "\n",
    "    #load disj image\n",
    "    img = image.load_img((impath) , target_size=(img_height, img_width))\n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "\n",
    "    #use model to predict classe\n",
    "    prediction = model.predict(img)\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(classes[np.argmax(score)], 100 * np.max(score)))\n",
    "    \n",
    "    #return clas and percentage of confidence\n",
    "    return [classes[np.argmax(score)], 100 * np.max(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model()\n",
    "\n",
    "result = classify_image(model, dataset.class_names, 'chart.png')\n",
    "\n",
    "tf.keras.models.save_model(model, classification_model_path)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3 - Captioning\n",
    "\n",
    "Ce livrable concerne la dernière étape du traitement requis. L’objectif est de créer un réseau de neurones qui génère des légendes pour des photographies, en s’appuyant sur le dataset dataset MS COCO. Le réseau sera composé de deux parties, la partie CNN qui encode les images en un représentation interne, et le partie RNN utilise cette représentation pour prédire l’annotation séquence par séquence. Avant l’entrainement du modèle les images sont prétraitées\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>L’architecture schématique complète du réseau utilisé pour le captioning explicitant le type de CNN utilisé pour les prétraitements.</li>\n",
    "    <li>Un petit descriptif sur le pré-traitements de images et du texte.</li>\n",
    "    <li>Le code explicitant l’architecture du CNN et du RNN utilisés dans le captioning.</li>\n",
    "    <li>L’évolution sous forme de courbes des performances du réseau pendant l’entrainement. Affichage de quelques exemples pour les tests.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 20/01/2021</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargez et préparez le jeu de données MS-COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_folder = '/Dataset/3/dataset'\n",
    "\n",
    "annotation_folder = coco_folder + '/annotations/'\n",
    "image_folder = coco_folder + '/train/'\n",
    "\n",
    "PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Attention : Cette partie permet de télécharger le jeu de donnée Coco si les dossiers ne dont pas deja présent dans le dataset !</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_folder = coco_folder + '/annotation/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.') + coco_folder,\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotation/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "image_folder = coco_folder + '/train/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.') + coco_folder,\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming directories\n",
    "if not os.path.exists(os.path.abspath('.') + '/Dataset/3/dataset/annotation'):\n",
    "    os.rename(os.path.abspath('.') + '/Dataset/3/dataset/annotations', os.path.abspath('.') + '/Dataset/3/dataset/annotation')\n",
    "    print(\"annotation folder Successfully renamed.\")\n",
    "    \n",
    "if not os.path.exists(os.path.abspath('.') + '/Dataset/3/dataset/train'):\n",
    "    os.rename(os.path.abspath('.') + '/Dataset/3/dataset/train2014', os.path.abspath('.') + '/Dataset/3/dataset/train')\n",
    "    print(\"train folder Successfully renamed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limite la taille de l'ensemble d'entraînement\n",
    "Pour accélérer la formation pour ce didacticiel, vous utiliserez un sous-ensemble de 30 000 légendes et leurs images correspondantes pour entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('.') + '/Dataset/3/dataset/annotation/captions_train2014.json', 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 10000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will \n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:10000]\n",
    "print(\"Nombre de path d'image enregistrés :\", len(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    train_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que l'image correspond bien à l'annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des images à l'aide d'InceptionV3\n",
    "\n",
    "Ensuite, on utilisere InceptionV3 pour classer chaque image. Vous extrairez des entités de la dernière couche convolutive.\n",
    "\n",
    "Tout d'abord, vous allez convertir les images au format attendu d'InceptionV3:\n",
    "<ul>\n",
    "    <li>Redimensionnement de l'image à 299 x 299 px</li>\n",
    "    <li>Prétraitez les images à l'aide de la méthode preprocess_input pour normaliser l'image afin qu'elle contienne des pixels compris entre -1 et 1, ce qui correspond au format des images utilisées pour entraîner InceptionV3.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3) #channels = (optional int) Defaults to 0. Number of color channels for the decoded image.\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialiser InceptionV3 et charger les poids Imagenet pré-entraînés\n",
    "\n",
    "Création d'un modèle **tf.keras** où la couche de sortie est la dernière couche convolutionnelle de l'architecture *InceptionV3*. La forme de la sortie de cette couche est *8 x 8 x 2048*. \n",
    "\n",
    "<ul>\n",
    "    <li>Vous transférez chaque image à travers le réseau et stockez le vecteur résultant dans un dictionnaire (image_name -> feature_vector).</li>\n",
    "    <li>Une fois que toutes les images sont passées sur le réseau, vous sélectionnez le dictionnaire et l'enregistrez sur le disque.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en cache des fonctionnalités extraites d'InceptionV3\n",
    "\n",
    "Vous pré-traiterez chaque image avec InceptionV3 et mettez en cache la sortie sur le disque. La mise en cache de la sortie est **8 * 8 * 2048** floats par image dans la RAM.\n",
    "\n",
    "*Les performances pourraient être améliorées avec une stratégie de mise en cache plus sophistiquée (par exemple, en partageant les images pour réduire les E / S disque à accès aléatoire), mais cela nécessiterait plus de code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ").batch(batch_size)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitez et jetez les sous-titres\n",
    "\n",
    "Tout d'abord, il faut tokeniser les légendes (par exemple, en les fractionnant sur des espaces). Cela nous donne un vocabulaire de tous les mots uniques dans les données.\n",
    "Ensuite, vous allez limiter la taille du vocabulaire aux **10 000 premiers mots** (pour économiser de la mémoire). Vous remplacerez tous les autres mots par le **jeton \"UNK\"** (inconnu).\n",
    "\n",
    "Ensuite, créer des mappages *mot-à-index* et *index-mot*.\n",
    "Enfin, on remplit toutes les séquences pour qu'elles aient la même longueur que la plus longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 10000 words from the vocabulary\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisez les données en formation et en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un ensemble de données **dataset** (tf.data) pour l'entraînement\n",
    "Nos images et légendes sont prêtes! Ensuite, créons un ensemble de données tf.data à utiliser pour entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle\n",
    "\n",
    "On commence par extraire les caractéristiques de la couche convolutive inférieure d'InceptionV3 en nous donnant un vecteur de forme (8 x 8 x 2048). Puis on écrase cela à une forme de (64 x 2048). \n",
    "\n",
    "Ce vecteur est ensuite passé à travers l'encodeur CNN (qui se compose d'une seule couche entièrement connectée). \n",
    "\n",
    "Et enfin le RNN (ici GRU) s'occupe de l'image pour prédire le mot suivant.\n",
    "\n",
    "Le modèle pourrait s'apparenter au fonctionnement suivant :\n",
    "\n",
    "<img src=\"imageSrc/model.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "**InceptionV3**\n",
    "\n",
    "Inception v3 est un modèle de reconnaissance d'images couramment utilisé qui a démontré, sur l'ensemble de données ImageNet, une justesse supérieure à 78,1 %.\n",
    "\n",
    "La principale différence entre les modèles Inception et les CNN classiques réside dans les blocs de démarrage. Celles-ci impliquent la convolution du même tenseur d'entrée avec plusieurs filtres et la concaténation de leurs résultats. Un tel bloc est représenté dans l'image ci-dessous. Ce sont donc des filtres de tailles multiples sur le même niveau. Au lieu de faire un système plus \"profond\", il est donc « plus large ». (les CNN standarts effectuent une seule opération de convolution sur chaque tenseur)\n",
    "\n",
    "<img src=\"imageSrc/conv.png\"/>\n",
    "\n",
    "Pour éviter que la partie du milieu du réseau ne «s'éteigne», les auteurs ont introduit un classificateurs auxiliaires. Il applique softmax à l’output et calcul le loss (la perte) sur les labels (utile à des fin de training)\n",
    "<img src=\"imageSrc/inceptionv3.png\"/>\n",
    "\n",
    "On récurère les images avant qu'elles atteignent la couche softmax ( la dernière couche) , au format vectoriel de dimention 8x8x2048.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dans cet exemple le captioning se fait de cette façon:\n",
    "\n",
    "L'image est passée à travers le CNN pour avoir une représentation compacte de celui-ci. Cette représentation est retournée par la couche `Dense 2` de taille 4096. Cette représentation est réduite en la passant à la couche dense `Dense Map` pour etre mise en entrée comme etat caché initial aux cellules du RNN.\n",
    "\n",
    "- La partie RNN est composée de GRU (Gated Reccurent Unit). Cette partie est constituée de 3 couches. Une couche représentant de manière assez sommaire un niveau d'abstraction du langage.\n",
    "\n",
    "- Le RNN a en entrée l'annotation ainsi que l'image en forme compacte, et retourne pour chaque colonne le mot suivant le mot en entrée au niveau de la colonne. Les annotations sont représentées en liste de mot. Cette liste est inexploitable par le RNN, elle est donc passée à un module qui remplace chaque mot par un entier (ou jeton entier), puis par un autre module qui projette chaque jeton en un vecteur dont les éléments sont entre -1 et 1. \n",
    "\n",
    "Votre système d'annotation suivra de manière assez globale, le même principe que montrée dans l'image ci-dessous, néanmoins il contiendra des différences essentielles le distinguant de cet exemple. La système contiendra, notamment, un mécanisme d'attention (expliqué dans article et dans la video) dont la fonction est d'amener le réseau de neurones à donner une plus grande importance dans ses prédictions de l'annotation aux parties de l'images les plus parlantes et les plus pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'encodeur CNN :**\n",
    "\n",
    "L'encodeur CNN produit une représentation adéquate de l'image qu'il transmet au décodeur RNN pour la légender. Le CNN a en entrée les caractéristiques des images déjà prétraitées par InceptionV3 et stockées sur disque. \n",
    "\n",
    "*Petite remarque*, dans la partie CNN de ce réseau de neurones, la dernière couche convolutive n’est pas aplatie. Rappelez-vous que les images issues du prétraitement par InceptionV3 étaient de la forme 8x8x2048. Ces images ont été remodelées pour avoir la taille 64x2048. \n",
    "\n",
    "Cela signifie que cette représentation contient pour chacune des 64 positions de l’image prétraitée les 2048 caractéristiques extraites par InveptoinV3. Et donc, l’entrée du décodeur CNN est un batch ou chaque élément est constitué des 2048 caractéristiques des 64 positions de l’image prétraitée (qui était à l’origine 8x8). La couche dense qui suit calcule une nouvelle représentation de l’image de taille 64x256 ou chaque position de l’image a donc 256 caractéristiques. Les poids sont les mêmes pour les neurones de mêmes position qui se trouvent sur la même colonne dans l’image prétraitée (qui sont associées à la même caractéristique de l’image). Ceci provient de la manière qu’a la [couche dense]( https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) de gérer les opérations matricielles dans tensorflow.\n",
    "\n",
    "L’avantage de cette représentation par rapport à la représentation aplatie est de préserver de l’information spatiale au niveau des couches du réseau de neurones. Ceci permettra au mécanisme d’attention de la partie RNN de détecter les positions intéressantes au niveau de l’image et de renseigner l’algorithme sur quelle zone il devra porter le plus d’importance pour légender l’image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le mécanisme d'attention :**\n",
    "\n",
    "Le mécanisme d’attention ressemble beaucoup à une cellule [RNN classique]( https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_r%C3%A9currents), mais avec quelques différences. La partie de l’attention a en entrée la représentation de l’image prétraitée retournée par le CNN ainsi que la valeur courante de l’état cachée du GRU, et en sortie le **vecteur du contexte** qui reflète les caractéristiques les plus importantes de l’image. Une étape intermédiaire pour calculer ce vecteur consiste à calculer les **poids d’attention** qui représentent l’importance de chaque position de l’image (il y en a 64) dans la prédiction de son annotation.\n",
    "\n",
    "La représentation de l’image donnée en entrée est transformée au début de la même manière que pour le CNN en la passant à une couche dense de taille `units`. De même, l’état caché est aussi passé à une couche dense de taille `units`. La nouvelle représentation de l’image est ensuite additionnée à l’état caché puis passée à une fonction d’activation de type [`tanh`](https://fr.wikipedia.org/wiki/Tangente_hyperbolique) comme pour les cellules classiques de RNN. \n",
    "\n",
    "À ce niveau-là, on aura une représentation des données de taille `64xunits` contenant un mélange d’informations sur l’image et sur le texte de l’annotation. Un score est ensuite associé à chacune des positions en passant cette représentation à une couche dense. Ces scores sont normalisés avec une couche softmax pour produire le vecteur des **poids d’attention**. \n",
    "Finalement, chaque caractéristique de la représentation de l’image en entrée sera multipliée (pondérée) par le vecteur d’attention. Après quoi, on prend la somme de chaque caractéristique le long des positions (les lignes de la représentation) pour former le **vecteur du contexte**.\n",
    "\n",
    "De façon globale, on peut dire que le vecteur d’attention dépend de scores qui sont appris à partir d’une représentation spatiale et textuelle de l’image. Ce vecteur d’attention renvoie la pertinence de chaque position et sert à calculer le vecteur du contexte qui nous donnera l’importance des caractéristiques de l’image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "        \n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le décodeur RNN :**\n",
    "\n",
    "Le rôle du décodeur RNN est d’utiliser la représentation prétraitée de l’image de prédire sa légende mot par mot. Ce RNN à une seule cellule de type [GRU]( https://en.wikipedia.org/wiki/Gated_recurrent_unit). Le GRU a un état caché qui représente la mémoire des derniers éléments vu par celui-ci. Le GRU met à jour son état avant de le retourner, pour cela il utilise certains mécanismes de mémorisation qui sont assez sophistiqués.\n",
    "\n",
    "Le décodeur est structuré comme suit, à chaque appel du RNN, le mot courant ainsi que la représentation de l’image et l’état caché du GRU sont donnés en entrée du RNN. Comme les mots sont représentés par des entiers, on doit faire passer ceux-ci par une couche dite [embedding layer]( https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) qui se chargera de calculer une représentation vectorielle de taille `output_dim` partant du nombre représentant le mot. \n",
    "\n",
    "À côté de ça, le mécanisme d’attention fournit un vecteur représentant **le contexte** de l’image c-à-d un vecteur qui nous renseigne sur les caractéristiques dominantes de l’image. Ce vecteur est calculé par un appel du mécanisme d’attention en lui fournissant en entrée les caractéristiques de l’image encodées par le CNN ainsi que l’état caché du GRU qui résume l’historique des mots vues par le RNN jusqu’à présent. \n",
    "\n",
    "Ensuite, le mot courant et le contexte sont concaténée pour former le vecteur d’entrée du GRU qui à son tour calcule l’état à l’étape suivante. Cet état est passée par une couche dense de taille `units` puis la sortie de cette couche est passée à une autre couche dense de taille `vocab_size` qui retourne le score associé à chaque mot du vocabulaire afin de prédire le mot suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point de contrôle\n",
    "\n",
    "Pour garder la trace de votre apprentissage et la sauvegarder, vous pouvez utiliser la classe tf.train.Checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# Initialisation de l'époque de début d’entrainement dans `start_epoch`. \n",
    "# La classe `tf.train.Checkpoint` permet de poursuivre l’entrainement là ou vous l’avez laissé s’il avait été interrompu auparavant.\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement et test :\n",
    "Vous implémenterez ensuite les fonctions `train_step` et `evaluate` :\n",
    "\n",
    "- La fonction `train_step` représente une étape de l'entrainement du réseau. Elle est composée de l'évaluation par l'encodeur du vecteur pré-calculé par InceptionV3. La sortie de cette étape sera transmise au décodeur qui se charge de prédire l'annotation mot par mot. La boucle pour prédire chaque mot et calculer la perte associée devra être implémenté dans cette fonction.\n",
    "- La fonction `evaluate` servira à évaluer les performances du réseau sur le jeu de test. Elle est donc similaire à la fonction `train_step` sauf que la partie calcul de la fonction de perte est absente car il n'agit pas d'entrainer le réseau.</li>\n",
    "\n",
    "Enfin, vous devez implémenter la partie du code qui fait l'entrainement et le test. Précisons que l'entrainement se fait ici par batch d'images.\n",
    "\n",
    "### Entrainement\n",
    "La fonction qui permet d'achever une étape d'entrainement sur un batch d'images est `train_step`. La fonction a en entrée un batch d'images prétraitées ainsi que leurs annotations et retourne la perte associée à ce batch. \n",
    "\n",
    "L'état caché de la partie RNN est initialisé ainsi que le mot de départ avec le token de début. Les caractéristiques de l'image sont ensuite extraites par l’encodeur. Après cela, on parcourt le batch mot par mot pour prédire le mot suivant à l'aide du décodeur. Le décodeur utilise l'état caché, les caractéristiques de l'image ainsi que le mot précédent pour prédire le mot courant. Le décodeur met à jour l'état caché et le retourne ainsi que les prédictions du batch. La perte est calculée à partir des prédictions retournées par le décodeur et les annotations associées au batch.\n",
    "\n",
    "Finalement, la perte globale ainsi que le gradient sont calculés et le réseau est mis à jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette boucle d'entrainement parcours le jeu de données d'entrainement batch par batch et entraine le réseaux avec ceux-ci.\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'\n",
    "                   .format(\n",
    "                       epoch + 1, \n",
    "                       batch, \n",
    "                       batch_loss.numpy() / int(target.shape[1])\n",
    "                   ))\n",
    "    \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    # print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'évaluation est similaire à la boucle d'apprentissage, sauf que vous n'utilisez pas le forçage de l'enseignant ici. L'entrée du décodeur à chaque pas de temps correspond à ses prédictions précédentes avec l'état caché et la sortie du codeur. Arrêtez de prédire quand le modèle prédit le jeton de fin. \n",
    "Et stockez les poids d'attention pour chaque pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques annotations dans le jeu de test\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picture_to_output(img, result):\n",
    "    # Remove the ''<end>' from the titles\n",
    "    result = filter(lambda x: x != '<end>', result)\n",
    "    # Remove the ''<unk>' from the titles\n",
    "    result = filter(lambda x: x != '<unk>', result)\n",
    "    # Remove the '\\n' from the titles\n",
    "    result = filter(lambda x: x != '\\n', result)\n",
    "    \n",
    "    # Move the image with his result as name to output folder\n",
    "    dest_path = captioning_output_path + '/' + img + ' - ' + ' '.join(result) + '.jpg'\n",
    "    shutil.copy(image_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode à utiliser pour sous-titrer les images avec le modèle que nous venons de former. Attention, il a été formé sur une quantité relativement petite de données et les images peuvent être différentes des données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the pictures into the \"input\" folder\n",
    "for img in os.listdir(captioning_input_path):\n",
    "    image_path = captioning_input_path + '/' + img\n",
    "    \n",
    "    # Evaluate the given image\n",
    "    result, attention_plot = evaluate(image_path)\n",
    "    print(img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "    # Give the details of the word founded\n",
    "    # plot_attention(image_path, result, attention_plot)\n",
    "\n",
    "    # Save image into output folder\n",
    "    save_picture_to_output(img, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single photo check\n",
    "img = random.choice(os.listdir(captioning_input_path))\n",
    "image_path = captioning_input_path + '/' + img\n",
    "\n",
    "# Evaluate the given image\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print (img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "# Save image into output folder\n",
    "save_picture_to_output(img, result)\n",
    "\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
