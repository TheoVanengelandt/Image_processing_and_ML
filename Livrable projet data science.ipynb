{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Livrable Projet DATA SCIENCE</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "L'entreprise TouNum est une entreprise de numérisation de documents. Elle prospose différents services dont la numérisation de base de document papier pour les entreprises clientes. TouNum veut optimiser et rendre intelligent ce processus de scanning en incluant des outils de Machine Learning. Le gain de temps serait important aux vues des nombreuses données que l'entreprise doit scanner et étiqueter.\n",
    "Pour cela, TouNum fait appel à CESI pour réaliser cette prestation.\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est que l'équipe de data scientist de CESI réalise cette solution visant à analyser des photographies pour en déterminer une légende descriptive de manière automatique. Il faudra également améliorer la qualité des images scannées ayant des qualités variables (parfois floues, ou bruitées).\n",
    "\n",
    "<img src=\"imageSrc/caption image.PNG\"/>\n",
    "\n",
    "### Enjeux\n",
    "\n",
    "TouNum devait trier et étiqueter chaque document scanné. La solution délivré par CESI permet l'automatisation de ces tâches en faisant donc gagner un temps non négligeable. Elle va donc pouvoir réaliser plus de contrats et augmenter la satisfaction client.\n",
    "\n",
    "### Contraintes techniques\n",
    "\n",
    "L'implémentation des algorithmes doit être réaliser sur Python, notamment les librairies Scikit et TensorFlow. La librairie Pandas doit être utilisé pour manipuler le dataset et ImageIO pour le charger. NumPy et MatPlotLib seront nécessaire pour le calcul scientifique et la modélisation.\n",
    "\n",
    "Le programme à livrer devra respecter le workflow suivant :\n",
    "\n",
    "<img src=\"imageSrc/workflow.PNG\"/>\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "La classification d'image se fera à l'aide de réseaux de neurones. Cette dernière doit distinguer les photos d'un autre documents, tel que schémas, textes scannés, voir peintures.\n",
    "TouNoum possède un dataset rempli d'images divers pour entrainer le réseau de neurones.\n",
    "\n",
    "#### Prétraitement\n",
    "\n",
    "Le prétraitement dois utiliser des filtres convolutifs afin d'améliorer la qualité. Il doit établir un compromis entre débruitage et affutage.\n",
    "\n",
    "#### Captionning\n",
    "\n",
    "Le Captionning devra légender automatiquement les images. Il utilisera deux techniques de Machine Learning : les réseaux de neurones convolutifs (CNN) pour prétraiter l'image en identifiant les zones d’intérêt, et les réseaux de neurones récurrents (RNN) pour générer les étiquettes. Il faudra être vigilant quant aux ressources RAM. Un dataset d'étiquetage classique est disponible pour l’apprentissage supervisé.\n",
    "\n",
    "### Livrable\n",
    "\n",
    "La solution doit sous forme de notebook Jupiter entièrement automatisé. Il doit être conçu pour être faciliter mis en production et maintenance.\n",
    "Il faut démontrer la pertinence du modèle de manière rigoureuse et pédagogique.\n",
    "\n",
    "#### Jalons\n",
    "\n",
    "CESI devra dois rendre le prototype complet et fonctionnel du programme pour le 23 janvier. \n",
    "TouNum exige également 3 dates de rendu pour suivre la bonne avancé du projet.\n",
    "<ul>\n",
    "    <li>18/12/20 : Prétraitement d'image</li>\n",
    "    <li>15/01/21 : Classification binaire</li>\n",
    "    <li>20/01/21 : Captioning d'images</li>\n",
    "    <li>22/01/21 : Démonstration </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable 1 - Prétraitement (denoising/sharpening…)\n",
    "\n",
    "Le but est de traiter un ensemble de photographies afin de les rendre mieux traitables par les algorithmes de Machine Learning. Il y a deux traitements à réaliser : le débruitage, et l’affutage. Vous devrez produire un notebook Jupyter explicitant ces étapes de prétraitement, et leurs performances. Ces algorithmes s’appuieront sur des notions assez simples autour des filtres de convolution, et les appliqueront pour améliorer la qualité de l’image. Il faudra notamment décider d’un compromis entre dé-bruitage et affutage.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code de chargement du fichier.</li>\n",
    "    <li>Le code du débruitage sur un sous-ensemble d’images bruitées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>Le code de l’affutage sur un sous-ensembles d’images floutées. Le code doit être accompagné d’explications.</li>\n",
    "    <li>\n",
    "        Une étude de cas explicitant les compromis entre ces deux opérations. Cette partie du livrable doit inclure le bruitage d’images et montrer la perte de détails, ou l’affutage d’images et montrer l’apparition du bruit.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/12/2020</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Importation des librairies utilisées*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Check if imageio package is installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio\n",
    "    \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if cikit-image package is installed\n",
    "try:\n",
    "    import skimage\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "from skimage import io\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "# Check if opencv-python package is installed\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    !pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Check if pandas package is installed\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "# Check if tensorflow package is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import tensorflow_datasets\n",
    "except ImportError:\n",
    "    !pip install tensorflow_datasets    \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    !pip install keras    \n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Livrable 3 \n",
    "import json\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Check if tqdm package is installed\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    !pip install -q tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Chemins physiques*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_dataset_path = \"./Dataset/1/dataset/Blurry/\"\n",
    "noisy_dataset_path = \"./Dataset/1/dataset/Noisy/\"\n",
    "deblured_dataset_path = \"./Dataset/1/dataset/deblurred/\"\n",
    "denoised_dataset_path = \"./Dataset/1/dataset/denoised/\"\n",
    "\n",
    "classification_dataset_path = \"./Dataset/2/dataset/\",\n",
    "classification_model_path = \"./Models/classification/\",\n",
    "\n",
    "captionning_dataset_path  = \"./Dataset/3/dataset/train/\",\n",
    "captionning_annotation_path  = \"./Dataset/3/dataset/annotation/\",\n",
    "captionning_model_path = \"./Models/captionning/\"\n",
    "\n",
    "captioning_input_path = './Dataset/3/input'\n",
    "captioning_output_path = './Dataset/3/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthodes utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des images\n",
    "def get_image(path, filename):\n",
    "    return io.imread(path + filename)\n",
    "\n",
    "# Sauvegarde des images\n",
    "def save_image(path, filename, content):\n",
    "    #Check if folder exists\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    imageio.imwrite(path + filename , content)\n",
    "\n",
    "# Retrieve importants informations from data    \n",
    "def get_metric_stat(pre_data, post_data):\n",
    "    data = [\n",
    "        np.array([min(pre_data), max(pre_data), np.median(pre_data), np.average(pre_data)]),\n",
    "        np.array([min(post_data), max(post_data), np.median(post_data), np.average(post_data)])\n",
    "    ]\n",
    "\n",
    "    data_array = pd.DataFrame(data,\n",
    "                              index = [\"pre_processed\", \"post_processed\"],\n",
    "                              columns = [\"Min value\", \"Max value\", \"Median value\", \"Average value\"])\n",
    "    print(data_array)\n",
    "\n",
    "# Display all data in table\n",
    "def get_list_data(data_tmp):\n",
    "    data_array = pd.DataFrame(np.array(data_tmp),\n",
    "                              columns = [\"Name\", \"Before\", \"After\"])\n",
    "    print(\"\\n\")\n",
    "    print(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défloutage de l'image\n",
    "\n",
    "Pour le défloutage les images, on passe par le filtrage via **convolution**. L'opération de convolution consiste à faire glisser une autre matrice nommée filtre (de taille généralement inférieure à l'image traitée) tout le long de l'image et remplacer la valeur de chaque pixel de l'image par la somme du produit des éléments de cette matrice.\n",
    "\n",
    "Les filtres d'amélioration de la netteté d'une image (ou filtres d'affutage de contours) permettent d'améliorer la qualité d'une image en accentuant les bords (ou en d'autres termes en accentuant les différences entres les pixels adjacents). L'affutage de contour consiste à prendre des différences.\n",
    "\n",
    "Pour le défloutage des images, on utilise un **filtre Laplacien**.\n",
    "Ce filtre nous permet d'affuter les images grâce à une fonction de convolution de la librairie opencv sur l'image récupérée.\n",
    "La variante de filtre choisie nous permet sur le jeu de données fourni d'affuter les images suffisamment pour retirer le flou présent sans pour autant y ajouter de bruit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication de la matrice Laplacienne\n",
    "\n",
    "\n",
    "L'approximation utilisée pour calculer le Laplacien est (si on prend la somme des l'approximation de la dérivée au sens des abscisses et des ordonnées) exprimée sous la forme $F(x+1,y)+F(x-1,y)+F(x,y+1)+F(x,y-1)-4F(x,y)$).\n",
    "\n",
    "Ce qui nous donne la matrice suivante et sa variante prenant en compte les diagonales (en effet, il existe une multitude de variantes):\n",
    "\n",
    "<img src=\"imageSrc/conv-laplacian.jpg\"/>\n",
    "\n",
    "La matrice de Laplace permet de mettre en évidence les contours d'une image comme on peut le voir selon l'image suivante:\n",
    "\n",
    "<img src=\"imageSrc/laplacian_filtered_image.jpg\"/>\n",
    "\n",
    "On voit donc mieux les contours, mais on perds le sens de l'image de départ. Pour corriger cela, on ajoute donc la matrice identitaire (l'image de départ) sur l'image d'arrivé, d'où le coefficient 9 au lieux de 8 (cf image ci dessous):\n",
    "\n",
    "<img src=\"imageSrc/laplace_conv_original.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblurring function\n",
    "def remove_blur(img, high):\n",
    "    kernel = []\n",
    "    \n",
    "    if high:\n",
    "        # Creation of a Laplacian kernel to use for debluring\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    else:\n",
    "        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    \n",
    "    # Convolution of the kernel with the image given in the function's parameter\n",
    "    return cv2.filter2D(img, -1, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation du niveau de flou\n",
    "\n",
    "Pour évaluer le niveau de flou, on utilise les contours Laplacien sur l'image (cf image ci dessus) et on évalue la variance de Laplace. Une variance faible indique qu'une faible plage de gris est utilisé (en d'autres termes, qu'il n'y a pas beaucoup de nuance utilisé, et donc qui se caractérise par du flou). Une variance élevé correspond donc à une large plage de gris utilisé (donc beaucoup plus de nuance disponible pour les détails). \n",
    "\n",
    "Il faut être vigilant à certaines images, tel qu'une photo du ciel, ou la variance sera faible même si cette dernière est net. L'indicateur devra alors être interprété en fonction du contexte.\n",
    "\n",
    "<img src=\"imageSrc/Laplace_Variance.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blurry_indicator(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(blurry_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "<ul>\n",
    "    <li> Utilisation de threads pour optimiser le temps d'éxecution </li>\n",
    "    <li> Calcul de la variance avant et après traitement </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = blurry_dataset_path + name\n",
    "    img = get_image(blurry_dataset_path,name)\n",
    "\n",
    "     # Get initial Blur metric\n",
    "    original_blur_metric = get_blurry_indicator(img)\n",
    "    pre_processed_data.append(original_blur_metric)\n",
    "    \n",
    "    # Remove blur from the colored image image\n",
    "    deblurred_img = remove_blur(img, high=True)\n",
    "\n",
    "    # Get initial Blur metric\n",
    "    processed_blur_metric = get_blurry_indicator(deblurred_img)\n",
    "    post_processed_data.append(processed_blur_metric)\n",
    "    \n",
    "    #print(\"image \" + name + \" - initial : \" + str(original_blur_metric)\n",
    "     #   + \" - processed : \" + str(processed_blur_metric)\n",
    "     #   + \" - difference : \" + str(processed_blur_metric - original_blur_metric)+\"\\n\")\n",
    "\n",
    "    data_preview_blurr.append([name, original_blur_metric, processed_blur_metric])\n",
    "\n",
    "    # Saving Image\n",
    "    save_image(deblured_dataset_path, name, deblurred_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_blurr = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_blurr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "def display_image_diff(originalPath, diffPath, filename=None):\n",
    "    if not filename:\n",
    "        # Get a random file from directory\n",
    "        filename = random.choice(os.listdir(originalPath)) \n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(get_image(originalPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    # Corrected Image noise\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(get_image(diffPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Corrected Image\")\n",
    "    \n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(blurry_dataset_path, deblured_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débruitage\n",
    "La capture d'un signal lumineux par un appareil photographique s'accompagne le plus souvent d'informations non désirées : le « bruit ». \n",
    "L'essentiel de ce « bruit » (des pixels trop clairs ou trop sombre en trop grand nombre ou de manière irrégulière, par exemple) est dû au capteur.\n",
    "\n",
    "### Le débruitage par morceaux (par patchs)\n",
    "Le débruitage par morceaux est une technique de débruitage d'image utilisant l'algorithme de réduction du bruit numérique appelé en Anglais \"non-local means\".\n",
    "La méthode repose sur un principe simple, remplacer la couleur d'un pixel par une moyenne des couleurs de pixels similaires. Mais les pixels les plus similaires à un pixel donné n'ont aucune raison d'être proches. Il est donc nécessaire de scanner une vaste partie de l'image à la recherche de tous les pixels qui ressemblent vraiment au pixel que l'on veut débruiter.\n",
    "\n",
    "### Pourquoi cette methode ?\n",
    "Le résultat d'un tel filtrage permet d’amoindrir la perte de détails au sein de l'image, comparé aux filtres réalisant des moyennes localement tel que le filtre de Gauss ou le filtre de Wiener, le bruit généré par l'algorithme \"non-local means\" est plus proche du bruit blanc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax:**\n",
    "cv2.fastNlMeansDenoisingColored( P1, P2, float P3, float P4, int P5, int P6)\n",
    "\n",
    "**Parameters:**\n",
    "* P1 – Source Image Array\n",
    "* P2 – Destination Image Array\n",
    "* P3 – Size in pixels of the template patch that is used to compute weights.\n",
    "* P4 – Size in pixels of the window that is used to compute a weighted average for the given pixel.\n",
    "* P5 – Parameter regulating filter strength for luminance component.\n",
    "* P6 – Same as above but for color components // Not used in a grayscale image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Noise function\n",
    "def remove_noise(image, high):\n",
    "    if high == 2:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "    elif high == 1:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 5, 10, 7, 15)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 15)\n",
    "\n",
    "def estimate_noise(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return estimate_sigma(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(noisy_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = noisy_dataset_path + name\n",
    "    img = get_image(noisy_dataset_path,name)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    original_noise_metric = estimate_noise(img)\n",
    "    pre_processed_data.append(original_noise_metric)\n",
    "    \n",
    "    denoised_img = remove_noise(img, high=2)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    processed_noise_metric = estimate_noise(denoised_img)\n",
    "    post_processed_data.append(processed_noise_metric)\n",
    "\n",
    "    data_preview_denoised.append([name, original_noise_metric, processed_noise_metric])\n",
    "    \n",
    "    save_image(denoised_dataset_path, name, denoised_img)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_denoised = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(noisy_dataset_path, denoised_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation entre Défloutage et Débruitage\n",
    "Afin d'améliorer l'image au maximum, on peut effectuer les 2 operations à savoir, le traitement de bruit et le traitement de flou, sur une même image et utiliser nos métriques de performances pour automatiser l'application des traitements. Un autre point d'importance, l'ordre dans lequel on effectue les traitements a un impact sur la qualité de l'image.\n",
    "\n",
    "### Procédé d'amelioration\n",
    "Afin de determiner la qualité générale d'une image en termes de bruit et de flou, on utilise la moyenne des mesures effectuées précedement sur les differents jeux de tests. En fonction de la première mesure de l'image, on décide de commencer par un débruitage ou un défloutage. Ensuite, une nouvelle mesure est effectuée et est comparée à nouveau avec les mesures des jeux de données post traitement. \n",
    "Si l'image n'est toujours pas considérées viable, on effectue l'opération inverse à plus faible intensité pour essayer d'avoir le meilleur compromis. Pour ces traitements, on se limite à deux passages pour éviter de détériorer l'image à analyser et pour éviter des temps de traitements trop longs. \n",
    "\n",
    "### Résultats\n",
    "Le résultat de ces tests a permis de demontrer que la meilleure solution consiste a commencer par effectuer un defloutage à forte intensité sur l'image puis, si c'est nécessaire, un debruitage à basse intensité. Cela représente le meilleur compromis au niveau de la qualité génerale de l'image.\n",
    "\n",
    "L'affichage de ces test est disponible ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose noisy or blurry image\n",
    "noisy = False\n",
    "\n",
    "img = []\n",
    "#if you want random testing\n",
    "#img = get_image(\"./Dataset/Blurry/\", random.choice(os.listdir(\"./Dataset/Blurry/\")))\n",
    "\n",
    "#image retrival\n",
    "if noisy:\n",
    "    img = get_image(noisy_dataset_path, \"noisy_117.jpg\")\n",
    "\n",
    "else:\n",
    "    img = get_image(blurry_dataset_path, \"blurry_092.jpg\")\n",
    "\n",
    "#initial image measurements\n",
    "initial_noise = estimate_noise(img)\n",
    "initial_blur = get_blurry_indicator(img)\n",
    "\n",
    "# print(initial_noise, initial_blur)\n",
    "\n",
    "#image is blurry\n",
    "if initial_blur < 3000:\n",
    "    # high deblur of the image\n",
    "    img_stage2 = remove_blur(img, high=False)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of noise\n",
    "    if second_noise < 1:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image doesn't meets requirements in terms of noise\n",
    "    else:\n",
    "        #low denoise of the image\n",
    "        img_stage3 = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        img_stage3_low_noise = remove_noise(img_stage2, 0)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 deblur and 1 low denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "#if image is noisy      \n",
    "if initial_noise > 1:\n",
    "    # high denoise of the image\n",
    "    img_stage2 = remove_noise(img, 2)\n",
    "    \n",
    "    img_stage2_low_noise = remove_noise(img, 1)\n",
    "    \n",
    "    #second image measurements\n",
    "    second_noise = estimate_noise(img)\n",
    "    second_blur = get_blurry_indicator(img)\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    if second_blur > 48000:\n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.subplot(123)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 deblur\")\n",
    "        plt.show()\n",
    "    \n",
    "    #image meets requirements in terms of blur\n",
    "    else:\n",
    "        #low deblur of the image\n",
    "        img_stage3 = remove_blur(img_stage2, high=False)\n",
    "        img_stage3_low_noise = remove_blur(img_stage2, high=False)\n",
    "        \n",
    "        #displaying images\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 denoise\")\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(img_stage2_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with only 1 medium denoise\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(img_stage3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 denoise and 1 low deblur\")\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(img_stage3_low_noise)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Corrected Image with 1 medium denoise and 1 low deblur\")\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "## Image\n",
    "\n",
    "\n",
    "## Défloutage\n",
    "\n",
    "* https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "* https://stackoverflow.com/questions/48319918/whats-the-theory-behind-computing-variance-of-an-image\n",
    "\n",
    "## Debruitage\n",
    "* https://docs.opencv.org/3.4/d5/d69/tutorial_py_non_local_means.html\n",
    "* http://www.ipol.im/pub/art/2011/bcm_nlm/article.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2 - Classification binaire\n",
    "\n",
    "L’entreprise voulant automatiser la sélection de photos pour l’annotations, le livrable 2 devra fournir une méthode de classification se basant sur les réseaux de neurones afin de filtrer les images qui ne sont pas des photos du dataset de départ.\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>Le code TensorFlow ainsi qu’un schéma de l’architecture du réseau de neurones. Toutes les parties doivent être détaillée dans le notebook : les paramètre du réseau, la fonction de perte ainsi que l’algorithme d’optimisation utilisé pour l’entrainement.</li>\n",
    "    <li>Un graphique contenant l’évolution de l’erreur d’entrainement ainsi que de l’erreur de test et l’évolution de l’accuracy pour ces deux datasets.</li>\n",
    "    <li>L’analyse de ces résultats, notamment le compromis entre biais et variance (ou sur-apprentissage et sous-apprentissage).</li>\n",
    "    <li>Une description des méthodes potentiellement utilisables pour améliorer les compromis biais/variance : technique de régularisation, drop out, early-stopping, …</li>\n",
    "</ul>\n",
    "\n",
    "Le but ultime est d’être capable de distinguer les photos parmi toutes ces images. Il est tout de même conseillé de commencer par les images les plus faciles à distinguer des photos, puis aller vers les dataset les plus difficiles à classifier (notamment, il y a dans le dataset peinture un certain nombre d’oeuvres au rendu assez réaliste, qui devraient vous poser problème).\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 18/01/2021</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics checks for image classifications\n",
    "print(\"executing tensorflow version \" + tf.__version__)\n",
    "if (len(tf.config.experimental.list_physical_devices('GPU')) == 1):\n",
    "    print(\"GPU is detected\")\n",
    "else :\n",
    "    print(\"GPU isn't detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the dataset (amount of images per batch, image resolution and training percentage)\n",
    "batch_size = 32\n",
    "img_height = 250\n",
    "img_width = 250\n",
    "validation_split = 0.3\n",
    "classes = ['Painting', 'Photo', 'Schematics', 'Sketch', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    #generation of the training dataset\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    #retrieve the amount of classes for the model\n",
    "    num_classes = len(train_ds.class_names)\n",
    "    print(\"Classes found : \" + str(num_classes))\n",
    "    print(train_ds.class_names)\n",
    "\n",
    "    #Allow for perfomance compilation times by preventing IO bottleneck on disks while compiling the model\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #Structure of the neural network\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.Conv2D(batch_size, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "    \n",
    "    #displat neural network structure\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "      optimizer='adam',\n",
    "      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    #amount of training and fitting\n",
    "    epochs=30\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "    \n",
    "    #display statitics over training accuracy\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def classify_image(model, classes, impath):\n",
    "\n",
    "    #load disj image\n",
    "    img = image.load_img((impath) , target_size=(img_height, img_width))\n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "\n",
    "    #use model to predict classe\n",
    "    prediction = model.predict(img)\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(classes[np.argmax(score)], 100 * np.max(score)))\n",
    "    \n",
    "    #return clas and percentage of confidence\n",
    "    return [classes[np.argmax(score)], 100 * np.max(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model()\n",
    "\n",
    "result = classify_image(model, dataset.class_names, 'chart.png')\n",
    "\n",
    "tf.keras.models.save_model(model, classification_model_path)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3 - Captioning\n",
    "\n",
    "Ce livrable concerne la dernière étape du traitement requis. L’objectif est de créer un réseau de neurones qui génère des légendes pour des photographies, en s’appuyant sur le dataset dataset MS COCO. Le réseau sera composé de deux parties, la partie CNN qui encode les images en un représentation interne, et le partie RNN utilise cette représentation pour prédire l’annotation séquence par séquence. Avant l’entrainement du modèle les images sont prétraitées\n",
    "\n",
    "Le notebook devra intégrer :\n",
    "<ul>\n",
    "    <li>L’architecture schématique complète du réseau utilisé pour le captioning explicitant le type de CNN utilisé pour les prétraitements.</li>\n",
    "    <li>Un petit descriptif sur le pré-traitements de images et du texte.</li>\n",
    "    <li>Le code explicitant l’architecture du CNN et du RNN utilisés dans le captioning.</li>\n",
    "    <li>L’évolution sous forme de courbes des performances du réseau pendant l’entrainement. Affichage de quelques exemples pour les tests.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Ce livrable est à fournir pour le 20/01/2021</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargez et préparez le jeu de données MS-COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_folder = '/Dataset/3/dataset'\n",
    "\n",
    "annotation_folder = coco_folder + '/annotations/'\n",
    "image_folder = coco_folder + '/train/'\n",
    "\n",
    "PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Attention : Cette partie permet de télécharger le jeu de donnée Coco !</font>\n",
    "\n",
    "Cette partie ne run pas si il y a deja les dossiers coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_folder = coco_folder + '/annotation/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.') + coco_folder,\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotation/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "image_folder = coco_folder + '/train/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.') + coco_folder,\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming directories\n",
    "if not os.path.exists(os.path.abspath('.') + '/Dataset/3/dataset/annotation'):\n",
    "    os.rename(os.path.abspath('.') + '/Dataset/3/dataset/annotations', os.path.abspath('.') + '/Dataset/3/dataset/annotation')\n",
    "    print(\"annotation folder Successfully renamed.\")\n",
    "    \n",
    "if not os.path.exists(os.path.abspath('.') + '/Dataset/3/dataset/train'):\n",
    "    os.rename(os.path.abspath('.') + '/Dataset/3/dataset/train2014', os.path.abspath('.') + '/Dataset/3/dataset/train')\n",
    "    print(\"train folder Successfully renamed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Facultatif:* limiter la taille de l'ensemble d'entraînement\n",
    "Pour accélérer la formation pour ce didacticiel, vous utiliserez un sous-ensemble de 30 000 légendes et leurs images correspondantes pour entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('.') + '/Dataset/3/dataset/annotation/captions_train2014.json', 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 10000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will \n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:10000]\n",
    "print(\"Nombre de path d'image enregistrés :\", len(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    train_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que l'image correspond bien à l'annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitez les images à l'aide d'InceptionV3\n",
    "\n",
    "Ensuite, vous utiliserez InceptionV3 pour classer chaque image. Vous extrairez des entités de la dernière couche convolutive.\n",
    "\n",
    "Tout d'abord, vous allez convertir les images au format attendu d'InceptionV3 soit:\n",
    "<ul>\n",
    "    <li>Redimensionnement de l'image à 299 x 299 px</li>\n",
    "    <li>Prétraitez les images à l'aide de la méthode preprocess_input pour normaliser l'image afin qu'elle contienne des pixels compris entre -1 et 1, ce qui correspond au format des images utilisées pour entraîner InceptionV3.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3) #channels = (optional int) Defaults to 0. Number of color channels for the decoded image.\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisez InceptionV3 et chargez les poids Imagenet pré-entraînés\n",
    "\n",
    "Création d'un modèle **tf.keras** où la couche de sortie est la dernière couche convolutionnelle de l'architecture *InceptionV3*. La forme de la sortie de cette couche est *8 x 8 x 2048*. \n",
    "\n",
    "<ul>\n",
    "    <li>Vous transférez chaque image à travers le réseau et stockez le vecteur résultant dans un dictionnaire (image_name -> feature_vector).</li>\n",
    "    <li>Une fois que toutes les images sont passées sur le réseau, vous sélectionnez le dictionnaire et l'enregistrez sur le disque.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en cache des fonctionnalités extraites d'InceptionV3\n",
    "\n",
    "Vous pré-traiterez chaque image avec InceptionV3 et mettez en cache la sortie sur le disque. La mise en cache de la sortie est **8 * 8 * 2048** floats par image dans la RAM.\n",
    "\n",
    "*Les performances pourraient être améliorées avec une stratégie de mise en cache plus sophistiquée (par exemple, en partageant les images pour réduire les E / S disque à accès aléatoire), mais cela nécessiterait plus de code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ").batch(batch_size)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitez et jetez les sous-titres\n",
    "\n",
    "Tout d'abord, il faut tokeniser les légendes (par exemple, en les fractionnant sur des espaces). Cela nous donne un vocabulaire de tous les mots uniques dans les données.\n",
    "Ensuite, vous allez limiter la taille du vocabulaire aux **5 000 premiers mots** (pour économiser de la mémoire). Vous remplacerez tous les autres mots par le **jeton \"UNK\"** (inconnu).\n",
    "\n",
    "Ensuite, créer des mappages *mot-à-index* et *index-mot*.\n",
    "Enfin, on remplit toutes les séquences pour qu'elles aient la même longueur que la plus longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisez les données en formation et en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un ensemble de données tf.data pour l'entraînement\n",
    "Nos images et légendes sont prêtes! Ensuite, créons un ensemble de données tf.data à utiliser pour entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, \n",
    "                      item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle\n",
    "\n",
    "On commence par extraire les caractéristiques de la couche convolutive inférieure d'InceptionV3 en nous donnant un vecteur de forme (8 x 8 x 2048). Puis on écrase cela à une forme de (64 x 2048). \n",
    "\n",
    "Ce vecteur est ensuite passé à travers l'encodeur CNN (qui se compose d'une seule couche entièrement connectée). \n",
    "\n",
    "Et enfin le RNN (ici GRU) s'occupe de l'image pour prédire le mot suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "        \n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point de contrôle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement\n",
    "<ul>\n",
    "    <li>Vous extrayez les fonctionnalités stockées dans les fichiers .npy respectifs, puis passez ces fonctionnalités via l'encodeur.</li>\n",
    "    <li>La sortie du codeur, l'état caché (initialisé à 0) et l'entrée du décodeur (qui est le jeton de démarrage) sont transmis au décodeur.</li>\n",
    "    <li>Le décodeur renvoie les prédictions et l'état caché du décodeur.</li>\n",
    "    <li>L'état caché du décodeur est ensuite renvoyé dans le modèle et les prédictions sont utilisées pour calculer la perte.</li>\n",
    "    <li>Utilisez le forçage de l'enseignant pour décider de la prochaine entrée du décodeur.</li>\n",
    "    <li>Le forçage par l'enseignant est la technique par laquelle le mot cible est transmis comme entrée suivante au décodeur.</li>\n",
    "    <li>La dernière étape consiste à calculer les dégradés et à l'appliquer à l'optimiseur et à la rétropropagation.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "            \n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'\n",
    "                   .format(\n",
    "                       epoch + 1, \n",
    "                       batch, \n",
    "                       batch_loss.numpy() / int(target.shape[1])\n",
    "                   ))\n",
    "    \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    # print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'évaluation est similaire à la boucle d'apprentissage, sauf que vous n'utilisez pas le forçage de l'enseignant ici. L'entrée du décodeur à chaque pas de temps correspond à ses prédictions précédentes avec l'état caché et la sortie du codeur. Arrêtez de prédire quand le modèle prédit le jeton de fin. \n",
    "Et stockez les poids d'attention pour chaque pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picture_to_output(img, result):\n",
    "        \n",
    "    # Remove the ''<end>' from the titles\n",
    "    result = filter(lambda x: x != '<end>', result)\n",
    "    # Remove the ''<unk>' from the titles\n",
    "    result = filter(lambda x: x != '<unk>', result)\n",
    "    # Remove the '\\n' from the titles\n",
    "    result = filter(lambda x: x != '\\n', result)\n",
    "    \n",
    "    # Move the image with his result as name to output folder\n",
    "    dest_path = captioning_output_path + '/' + img + ' - ' + ' '.join(result) + '.jpg'\n",
    "    shutil.copy(image_path, dest_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le plaisir, nous avons fourni ci-dessous une méthode que vous pouvez utiliser pour sous-titrer vos propres images avec le modèle que nous venons de former. \n",
    "Gardez à l'esprit qu'il a été formé sur une quantité relativement petite de données et que vos images peuvent être différentes des données d'entraînement (alors préparez-vous à des résultats étranges!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image used by the tensoflow exemple\n",
    "# image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "# image_extension = image_url[-4:]\n",
    "# image_path = tf.keras.utils.get_file('image' + image_extension, origin=image_url)\n",
    "\n",
    "for img in os.listdir(captioning_input_path):\n",
    "    image_path = captioning_input_path + '/' + img\n",
    "    \n",
    "    # Evaluate the given image\n",
    "    result, attention_plot = evaluate(image_path)\n",
    "    print(img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "    # Give the details of the word founded\n",
    "    # plot_attention(image_path, result, attention_plot)\n",
    "    \n",
    "    save_picture_to_output(img, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single photo check\n",
    "\n",
    "img = random.choice(os.listdir(captioning_input_path))\n",
    "image_path = captioning_input_path + '/' + img\n",
    "\n",
    "# Evaluate the given image\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print (img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "# Save image into output\n",
    "save_picture_to_output(img, result)\n",
    "\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
