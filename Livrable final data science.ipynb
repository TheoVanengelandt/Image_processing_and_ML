{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable final data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "L'entreprise TouNum est une entreprise de numérisation de documents. Elle prospose différents services dont la numérisation de base de document papier pour les entreprises clientes. TouNum veut optimiser et rendre intelligent ce processus de scanning en incluant des outils de Machine Learning. Le gain de temps serait important aux vues des nombreuses données que l'entreprise doit scanner et étiqueter.\n",
    "Pour cela, TouNum fait appel à CESI pour réaliser cette prestation.\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est que l'équipe de data scientist de CESI réalise cette solution visant à analyser des photographies pour en déterminer une légende descriptive de manière automatique. Il faudra également améliorer la qualité des images scannées ayant des qualités variables (parfois floues, ou bruitées).\n",
    "\n",
    "<img src=\"imageSrc/caption image.PNG\"/>\n",
    "\n",
    "### Enjeux\n",
    "\n",
    "TouNum devait trier et étiqueter chaque document scanné. La solution délivré par CESI permet l'automatisation de ces tâches en faisant donc gagner un temps non négligeable. Elle va donc pouvoir réaliser plus de contrats et augmenter la satisfaction client.\n",
    "\n",
    "### Contraintes techniques\n",
    "\n",
    "L'implémentation des algorithmes doit être réaliser sur Python, notamment les librairies Scikit et TensorFlow. La librairie Pandas doit être utilisé pour manipuler le dataset et ImageIO pour le charger. NumPy et MatPlotLib seront nécessaire pour le calcul scientifique et la modélisation.\n",
    "\n",
    "Le programme à livrer devra respecter le workflow suivant :\n",
    "\n",
    "<img src=\"imageSrc/workflow.PNG\"/>\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "La classification d'image se fera à l'aide de réseaux de neurones. Cette dernière doit distinguer les photos d'un autre documents, tel que schémas, textes scannés, voir peintures.\n",
    "TouNoum possède un dataset rempli d'images divers pour entrainer le réseau de neurones.\n",
    "\n",
    "#### Prétraitement\n",
    "\n",
    "Le prétraitement dois utiliser des filtres convolutifs afin d'améliorer la qualité. Il doit établir un compromis entre débruitage et affutage.\n",
    "\n",
    "#### Captionning\n",
    "\n",
    "Le Captionning devra légender automatiquement les images. Il utilisera deux techniques de Machine Learning : les réseaux de neurones convolutifs (CNN) pour prétraiter l'image en identifiant les zones d’intérêt, et les réseaux de neurones récurrents (RNN) pour générer les étiquettes. Il faudra être vigilant quant aux ressources RAM. Un dataset d'étiquetage classique est disponible pour l’apprentissage supervisé.\n",
    "\n",
    "### Livrable\n",
    "\n",
    "La solution doit sous forme de notebook Jupiter entièrement automatisé. Il doit être conçu pour être faciliter mis en production et maintenance.\n",
    "Il faut démontrer la pertinence du modèle de manière rigoureuse et pédagogique.\n",
    "\n",
    "#### Jalons\n",
    "\n",
    "CESI devra dois rendre le prototype complet et fonctionnel du programme pour le 23 janvier. \n",
    "TouNum exige également 3 dates de rendu pour suivre la bonne avancé du projet.\n",
    "<ul>\n",
    "    <li>18/12/20 : Prétraitement d'image</li>\n",
    "    <li>15/01/21 : Classification binaire</li>\n",
    "    <li>20/01/21 : Captioning d'images</li>\n",
    "    <li>22/01/21 : Démonstration </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Importation des librairies utilisées*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Check if imageio package is installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio\n",
    "    \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if cikit-image package is installed\n",
    "try:\n",
    "    import skimage\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "from skimage import io\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "# Check if opencv-python package is installed\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    !pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Check if pandas package is installed\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Livrable 2\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "# Check if tensorflow package is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import tensorflow_datasets\n",
    "except ImportError:\n",
    "    !pip install tensorflow_datasets    \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    !pip install keras    \n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "\n",
    "# Livrable 3 \n",
    "import json\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Check if tqdm package is installed\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    !pip install -q tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Chemins physiques*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Livrable 2\n",
    "classification_dataset_path = \"./Dataset/2/dataset/\"\n",
    "classification_model_path = \"./Models/classification/\"\n",
    "classification_input_path = './Dataset/2/input/'\n",
    "classification_output_path = './Dataset/2/output/'\n",
    "\n",
    "# Livrable 1\n",
    "blurry_dataset_path = \"./Dataset/1/dataset/Blurry/\"\n",
    "noisy_dataset_path = \"./Dataset/1/dataset/Noisy/\"\n",
    "deblured_dataset_path = \"./Dataset/1/dataset/deblurred/\"\n",
    "denoised_dataset_path = \"./Dataset/1/dataset/denoised/\"\n",
    "treatments_output_path = './Dataset/1/output/'\n",
    "\n",
    "#Livrable 3\n",
    "captioning_output_path = './Dataset/3/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable 2 - Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing tensorflow version 2.3.0\n",
      "GPU is detected\n"
     ]
    }
   ],
   "source": [
    "#basics checks for image classifications\n",
    "print(\"executing tensorflow version \" + tf.__version__)\n",
    "\n",
    "if (len(tf.config.experimental.list_physical_devices('GPU')) == 1):\n",
    "    print(\"GPU is detected\")\n",
    "else :\n",
    "    print(\"GPU isn't detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the dataset (amount of images per batch, image resolution and training percentage)\n",
    "batch_size = 16\n",
    "img_height = 200\n",
    "img_width = 200\n",
    "validation_split = 0.2\n",
    "\n",
    "epochs=15\n",
    "classes = ['Other', 'Photo']\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def generate_model():\n",
    "    #generation of the training dataset\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    #retrieve the amount of classes for the model\n",
    "    num_classes = len(train_ds.class_names)\n",
    "    print(\"Classes found : \" + str(num_classes))\n",
    "    print(train_ds.class_names)\n",
    "\n",
    "    #Allow for perfomance compilation times by preventing IO bottleneck on disks while compiling the model\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #Structure of the neural network\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.Conv2D(16, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(64, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dropout(0.2),\n",
    "      layers.Dense(2)\n",
    "    ])\n",
    "    \n",
    "    #display neural network structure\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "        #metrics=['accuracy', get_f1])\n",
    "    \n",
    "    #amount of training and fitting\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "    \n",
    "    #display statitics over training accuracy\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def classify_image(model, classes, impath):\n",
    "    #load disj image\n",
    "    img = image.load_img((impath), target_size=(img_height, img_width))\n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "\n",
    "    #use model to predict classe\n",
    "    prediction = model.predict(img)\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    #     print(\n",
    "    #         \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    #         .format(classes[np.argmax(score)], 100 * np.max(score))\n",
    "    #     )\n",
    "    #return clas and percentage of confidence\n",
    "    return [classes[np.argmax(score)], 100 * np.max(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(newModel=False):\n",
    "    # Checking if the Models folder is empty\n",
    "    if newModel == True: \n",
    "        model = generate_model()\n",
    "        tf.keras.models.save_model(model, classification_model_path)\n",
    "        return model    \n",
    "    # Or not\n",
    "    else:\n",
    "        return tf.keras.models.load_model(classification_model_path)\n",
    "    \n",
    "model = get_model(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "013_1_1_sz1.jpg Other 98.37130308151245 %\n",
      "047_1_1_sz1.jpg Other 98.12994599342346 %\n",
      "128.png Other 100.0 %\n",
      "129.png Other 100.0 %\n",
      "134_1_1_sz1.jpg Other 96.56375050544739 %\n",
      "42.png Other 99.99719858169556 %\n",
      "photo_0077.jpg Photo 71.0858166217804 %\n",
      "photo_0193.jpg Photo 92.31694340705872 %\n",
      "photo_0197.jpg Photo 95.69118618965149 %\n",
      "photo_0292.jpg Photo 73.42495918273926 %\n",
      "photo_0364.jpg Photo 81.79098963737488 %\n",
      "photo_0499.jpg Photo 99.46569204330444 %\n",
      "photo_0619.jpg Photo 99.99792575836182 %\n"
     ]
    }
   ],
   "source": [
    "# confidence threshold about consider a image as a photo\n",
    "confidence_threshold = 60\n",
    "\n",
    "# Classify images and moved \"Photo\" to the output folder \n",
    "for pictures in os.listdir(classification_input_path):\n",
    "    res_classe, res_score = classify_image(model, classes, os.path.join(classification_input_path,pictures))\n",
    "    \n",
    "    print(pictures, res_classe, res_score, '%')\n",
    "\n",
    "    if (res_classe == 'Photo' and res_score > confidence_threshold):\n",
    "        shutil.copy2(classification_input_path + \"/\" + pictures, classification_output_path) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - Prétraitement (denoising/sharpening…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblurring function\n",
    "def remove_blur(img, high):\n",
    "    kernel = []\n",
    "    \n",
    "    if high:\n",
    "        # Creation of a Laplacian kernel to use for debluring\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    else:\n",
    "        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    \n",
    "    # Convolution of the kernel with the image given in the function's parameter\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def get_blurry_indicator(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    return fm\n",
    "\n",
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = classification_output_path + name\n",
    "    img = get_image(classification_output_path,name)\n",
    "\n",
    "     # Get initial Blur metric\n",
    "    #original_blur_metric = get_blurry_indicator(img)\n",
    "    #pre_processed_data.append(original_blur_metric)\n",
    "    \n",
    "    # Remove blur from the colored image image\n",
    "    deblurred_img = remove_blur(img, high=True)\n",
    "\n",
    "    # Get initial Blur metric\n",
    "    #processed_blur_metric = get_blurry_indicator(deblurred_img)\n",
    "    #post_processed_data.append(processed_blur_metric)\n",
    "    \n",
    "    #print(\"image \" + name + \" - initial : \" + str(original_blur_metric)\n",
    "     #   + \" - processed : \" + str(processed_blur_metric)\n",
    "     #   + \" - difference : \" + str(processed_blur_metric - original_blur_metric)+\"\\n\")\n",
    "\n",
    "    #data_preview_blurr.append([name, original_blur_metric, processed_blur_metric])\n",
    "\n",
    "    # Saving Image\n",
    "    save_image(treatments_output_path, name, deblurred_img)\n",
    "    \n",
    "# Remove Noise function\n",
    "def remove_noise(image, high):\n",
    "    if high == 2:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "    elif high == 1:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 5, 10, 7, 15)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 15)\n",
    "\n",
    "def estimate_noise(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return estimate_sigma(img)\n",
    "\n",
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = noisy_dataset_path + name\n",
    "    img = get_image(noisy_dataset_path,name)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    original_noise_metric = estimate_noise(img)\n",
    "    pre_processed_data.append(original_noise_metric)\n",
    "    \n",
    "    denoised_img = remove_noise(img, high=2)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    processed_noise_metric = estimate_noise(denoised_img)\n",
    "    post_processed_data.append(processed_noise_metric)\n",
    "\n",
    "    data_preview_denoised.append([name, original_noise_metric, processed_noise_metric])\n",
    "    \n",
    "    save_image(denoised_dataset_path, name, denoised_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation entre Défloutage et Débruitage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path, filename):\n",
    "    return io.imread(path + filename)\n",
    "\n",
    "def save_image(path, filename, content):\n",
    "    #Check if folder exists\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    imageio.imwrite(path + filename , content)\n",
    "\n",
    "def clear_image(img):\n",
    "        \n",
    "    #if you want random testing\n",
    "    img = get_image(classification_output_path, img)\n",
    "\n",
    "    #initial image measurements\n",
    "    initial_noise = estimate_noise(img)\n",
    "    initial_blur = get_blurry_indicator(img)\n",
    "    # print(initial_noise, initial_blur)\n",
    "    #print(initial_noise)\n",
    "    #print(initial_blur)\n",
    "    #image is blurry\n",
    "    if initial_blur < 3000:\n",
    "        print('too much blur')\n",
    "        # high deblur of the image\n",
    "        img = remove_blur(img, high=False)\n",
    "\n",
    "        #second image measurements\n",
    "        second_noise = estimate_noise(img)\n",
    "        second_blur = get_blurry_indicator(img)\n",
    "\n",
    "        #image doesn't meets requirements in terms of noise\n",
    "        if second_noise > 2:\n",
    "            #low denoise of the image\n",
    "            img = remove_noise(img, 0)\n",
    "\n",
    "            img = remove_noise(img, 0)\n",
    "\n",
    "    #if image is noisy      \n",
    "    elif initial_noise > 0.8:\n",
    "        print('too much noise')\n",
    "        # high denoise of the image\n",
    "        img = remove_noise(img, 1)\n",
    "\n",
    "        #second image measurements\n",
    "        second_noise = estimate_noise(img)\n",
    "        second_blur = get_blurry_indicator(img)\n",
    "\n",
    "        #image meets requirements in terms of blur\n",
    "        if second_blur < 48000:\n",
    "            #low deblur of the image\n",
    "            img = remove_blur(img, high=False)\n",
    "            \n",
    "    print('Image cleaning is done ;)')\n",
    "    \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "too much noise\n",
      "Image cleaning is done ;)\n",
      "too much blur\n",
      "Image cleaning is done ;)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "for pictures in os.listdir(classification_output_path):\n",
    "    c_img = clear_image(pictures)\n",
    "    \n",
    "    save_image(treatments_output_path, pictures, c_img)\n",
    "\n",
    "    #shutil.copy2(classification_input_path + pictures, treatments_output_path)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3 - Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de path d'image enregistrés : 10000\n",
      "<start> A handicapped public toilet with rails and bars. <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 625/625 [01:12<00:00,  8.62it/s]\n"
     ]
    }
   ],
   "source": [
    "annotation_folder = './Dataset/3/dataset/annotation/'\n",
    "image_folder = './Dataset/3/dataset/train/'\n",
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "with open(os.path.abspath('.') + '/Dataset/3/dataset/annotation/captions_train2014.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = './Dataset/3/dataset/train/' + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 10000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will \n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:10000]\n",
    "print(\"Nombre de path d'image enregistrés :\", len(train_image_paths))\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    train_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))\n",
    "    \n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3) #channels = (optional int) Defaults to 0. Number of color channels for the decoded image.\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ").batch(batch_size)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())\n",
    "        \n",
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 10000 words from the vocabulary\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n",
    "\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n",
    "\n",
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "        \n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x000001320BB49280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function train_step at 0x000001320BB49280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x0000013007DAF520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x0000013007DAF520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1 Batch 0 Loss 2.2636\n",
      "Epoch 1 Batch 100 Loss 1.2104\n",
      "Epoch 1 Batch 200 Loss 1.0930\n",
      "Epoch 1 Batch 300 Loss 0.8979\n",
      "Epoch 1 Batch 400 Loss 0.8675\n",
      "Epoch 1 Batch 500 Loss 0.8973\n",
      "Epoch 1 Batch 600 Loss 0.7831\n",
      "Time taken for 1 epoch 214.69044423103333 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8856\n",
      "Epoch 2 Batch 100 Loss 0.8190\n",
      "Epoch 2 Batch 200 Loss 0.7667\n",
      "Epoch 2 Batch 300 Loss 0.7528\n",
      "Epoch 2 Batch 400 Loss 0.8069\n",
      "Epoch 2 Batch 500 Loss 0.7644\n",
      "Epoch 2 Batch 600 Loss 0.6732\n",
      "Time taken for 1 epoch 130.83101058006287 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7553\n",
      "Epoch 3 Batch 100 Loss 0.6934\n",
      "Epoch 3 Batch 200 Loss 0.7017\n",
      "Epoch 3 Batch 300 Loss 0.7195\n",
      "Epoch 3 Batch 400 Loss 0.7190\n",
      "Epoch 3 Batch 500 Loss 0.7396\n",
      "Epoch 3 Batch 600 Loss 0.6096\n",
      "Time taken for 1 epoch 131.31424474716187 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6673\n",
      "Epoch 4 Batch 100 Loss 0.7152\n",
      "Epoch 4 Batch 200 Loss 0.7131\n",
      "Epoch 4 Batch 300 Loss 0.6533\n",
      "Epoch 4 Batch 400 Loss 0.6160\n",
      "Epoch 4 Batch 500 Loss 0.6770\n",
      "Epoch 4 Batch 600 Loss 0.6734\n",
      "Time taken for 1 epoch 133.3767910003662 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6332\n",
      "Epoch 5 Batch 100 Loss 0.6024\n",
      "Epoch 5 Batch 200 Loss 0.6408\n",
      "Epoch 5 Batch 300 Loss 0.6506\n",
      "Epoch 5 Batch 400 Loss 0.5750\n",
      "Epoch 5 Batch 500 Loss 0.6028\n",
      "Epoch 5 Batch 600 Loss 0.5847\n",
      "Time taken for 1 epoch 127.04650092124939 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6239\n",
      "Epoch 6 Batch 100 Loss 0.6032\n",
      "Epoch 6 Batch 200 Loss 0.5081\n",
      "Epoch 6 Batch 300 Loss 0.5705\n",
      "Epoch 6 Batch 400 Loss 0.5453\n",
      "Epoch 6 Batch 500 Loss 0.5405\n",
      "Epoch 6 Batch 600 Loss 0.5270\n",
      "Time taken for 1 epoch 129.88218784332275 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5633\n",
      "Epoch 7 Batch 100 Loss 0.5152\n",
      "Epoch 7 Batch 200 Loss 0.5690\n",
      "Epoch 7 Batch 300 Loss 0.5468\n",
      "Epoch 7 Batch 400 Loss 0.5701\n",
      "Epoch 7 Batch 500 Loss 0.5635\n",
      "Epoch 7 Batch 600 Loss 0.5495\n",
      "Time taken for 1 epoch 128.08000087738037 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5383\n",
      "Epoch 8 Batch 100 Loss 0.5322\n",
      "Epoch 8 Batch 200 Loss 0.4867\n",
      "Epoch 8 Batch 300 Loss 0.5380\n",
      "Epoch 8 Batch 400 Loss 0.5028\n",
      "Epoch 8 Batch 500 Loss 0.5154\n",
      "Epoch 8 Batch 600 Loss 0.5020\n",
      "Time taken for 1 epoch 127.87999868392944 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5213\n",
      "Epoch 9 Batch 100 Loss 0.4545\n",
      "Epoch 9 Batch 200 Loss 0.4719\n",
      "Epoch 9 Batch 300 Loss 0.4892\n",
      "Epoch 9 Batch 400 Loss 0.5122\n",
      "Epoch 9 Batch 500 Loss 0.4687\n",
      "Epoch 9 Batch 600 Loss 0.4530\n",
      "Time taken for 1 epoch 134.49850010871887 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.5481\n",
      "Epoch 10 Batch 100 Loss 0.4693\n",
      "Epoch 10 Batch 200 Loss 0.4614\n",
      "Epoch 10 Batch 300 Loss 0.4733\n",
      "Epoch 10 Batch 400 Loss 0.4717\n",
      "Epoch 10 Batch 500 Loss 0.4852\n",
      "Epoch 10 Batch 600 Loss 0.4113\n",
      "Time taken for 1 epoch 132.537504196167 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# Initialisation de l'époque de début d’entrainement dans `start_epoch`. \n",
    "# La classe `tf.train.Checkpoint` permet de poursuivre l’entrainement là ou vous l’avez laissé s’il avait été interrompu auparavant.\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "# Cette boucle d'entrainement parcours le jeu de données d'entrainement batch par batch et entraine le réseaux avec ceux-ci.\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'\n",
    "                   .format(\n",
    "                       epoch + 1, \n",
    "                       batch, \n",
    "                       batch_loss.numpy() / int(target.shape[1])\n",
    "                   ))\n",
    "    \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    # print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picture_to_output(img, result):\n",
    "    # Remove the ''<end>' from the titles\n",
    "    result = filter(lambda x: x != '<end>', result)\n",
    "    # Remove the ''<unk>' from the titles\n",
    "    result = filter(lambda x: x != '<unk>', result)\n",
    "    # Remove the '\\n' from the titles\n",
    "    result = filter(lambda x: x != '\\n', result)\n",
    "    \n",
    "    # Move the image with his result as name to output folder\n",
    "    dest_path = captioning_output_path + '/' + img + ' - ' + ' '.join(result) + '.jpg'\n",
    "    shutil.copy(image_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo_0077.jpg Prediction Caption: a computer mouse sits in front of a passenger computer <end>\n",
      "photo_0193.jpg Prediction Caption: a person on the air floating down the water skiing <end>\n",
      "photo_0197.jpg Prediction Caption: a person riding a snowboard in the snow on a long snowboard on top of a steep mountain <end>\n",
      "photo_0292.jpg Prediction Caption: a woman wearing a pair of food <end>\n",
      "photo_0364.jpg Prediction Caption: two giraffes eating food <end>\n",
      "photo_0499.jpg Prediction Caption: a basketball player in red t shirt runs with tennis racket <end>\n",
      "photo_0619.jpg Prediction Caption: the train is traveling on town next to a lot <end>\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all the pictures into the \"input\" folder\n",
    "for img in os.listdir(treatments_output_path):\n",
    "    image_path = treatments_output_path + '/' + img\n",
    "    \n",
    "    # Evaluate the given image\n",
    "    result, attention_plot = evaluate(image_path)\n",
    "    print(img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "    # Give the details of the word founded\n",
    "    # plot_attention(image_path, result, attention_plot)\n",
    "\n",
    "    # Save image into output folder\n",
    "    save_picture_to_output(img, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
