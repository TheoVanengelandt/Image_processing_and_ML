{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable final data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "L'entreprise TouNum est une entreprise de numérisation de documents. Elle prospose différents services dont la numérisation de base de document papier pour les entreprises clientes. TouNum veut optimiser et rendre intelligent ce processus de scanning en incluant des outils de Machine Learning. Le gain de temps serait important aux vues des nombreuses données que l'entreprise doit scanner et étiqueter.\n",
    "Pour cela, TouNum fait appel à CESI pour réaliser cette prestation.\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est que l'équipe de data scientist de CESI réalise cette solution visant à analyser des photographies pour en déterminer une légende descriptive de manière automatique. Il faudra également améliorer la qualité des images scannées ayant des qualités variables (parfois floues, ou bruitées).\n",
    "\n",
    "<img src=\"imageSrc/caption image.PNG\"/>\n",
    "\n",
    "### Enjeux\n",
    "\n",
    "TouNum devait trier et étiqueter chaque document scanné. La solution délivré par CESI permet l'automatisation de ces tâches en faisant donc gagner un temps non négligeable. Elle va donc pouvoir réaliser plus de contrats et augmenter la satisfaction client.\n",
    "\n",
    "### Contraintes techniques\n",
    "\n",
    "L'implémentation des algorithmes doit être réaliser sur Python, notamment les librairies Scikit et TensorFlow. La librairie Pandas doit être utilisé pour manipuler le dataset et ImageIO pour le charger. NumPy et MatPlotLib seront nécessaire pour le calcul scientifique et la modélisation.\n",
    "\n",
    "Le programme à livrer devra respecter le workflow suivant :\n",
    "\n",
    "<img src=\"imageSrc/workflow.PNG\"/>\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "La classification d'image se fera à l'aide de réseaux de neurones. Cette dernière doit distinguer les photos d'un autre documents, tel que schémas, textes scannés, voir peintures.\n",
    "TouNoum possède un dataset rempli d'images divers pour entrainer le réseau de neurones.\n",
    "\n",
    "#### Prétraitement\n",
    "\n",
    "Le prétraitement dois utiliser des filtres convolutifs afin d'améliorer la qualité. Il doit établir un compromis entre débruitage et affutage.\n",
    "\n",
    "#### Captionning\n",
    "\n",
    "Le Captionning devra légender automatiquement les images. Il utilisera deux techniques de Machine Learning : les réseaux de neurones convolutifs (CNN) pour prétraiter l'image en identifiant les zones d’intérêt, et les réseaux de neurones récurrents (RNN) pour générer les étiquettes. Il faudra être vigilant quant aux ressources RAM. Un dataset d'étiquetage classique est disponible pour l’apprentissage supervisé.\n",
    "\n",
    "### Livrable\n",
    "\n",
    "La solution doit sous forme de notebook Jupiter entièrement automatisé. Il doit être conçu pour être faciliter mis en production et maintenance.\n",
    "Il faut démontrer la pertinence du modèle de manière rigoureuse et pédagogique.\n",
    "\n",
    "#### Jalons\n",
    "\n",
    "CESI devra dois rendre le prototype complet et fonctionnel du programme pour le 23 janvier. \n",
    "TouNum exige également 3 dates de rendu pour suivre la bonne avancé du projet.\n",
    "<ul>\n",
    "    <li>18/12/20 : Prétraitement d'image</li>\n",
    "    <li>15/01/21 : Classification binaire</li>\n",
    "    <li>20/01/21 : Captioning d'images</li>\n",
    "    <li>22/01/21 : Démonstration </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Importation des librairies utilisées*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Check if imageio package is installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio\n",
    "    \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if cikit-image package is installed\n",
    "try:\n",
    "    import skimage\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "from skimage import io\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "# Check if opencv-python package is installed\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    !pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Check if pandas package is installed\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Livrable 2\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "# Check if tensorflow package is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import tensorflow_datasets\n",
    "except ImportError:\n",
    "    !pip install tensorflow_datasets    \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Check if tensorflow_datasets package is installed\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    !pip install keras    \n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "\n",
    "# Livrable 3 \n",
    "import json\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Check if tqdm package is installed\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    !pip install -q tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Chemins physiques*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Livrable 2\n",
    "classification_dataset_path = \"./Dataset/2/dataset/\"\n",
    "classification_model_path = \"./Models/classification/\"\n",
    "classification_input_path = './Dataset/2/input'\n",
    "classification_output_path = './Dataset/2/output'\n",
    "\n",
    "# Livrable 1\n",
    "blurry_dataset_path = \"./Dataset/1/dataset/Blurry/\"\n",
    "noisy_dataset_path = \"./Dataset/1/dataset/Noisy/\"\n",
    "deblured_dataset_path = \"./Dataset/1/dataset/deblurred/\"\n",
    "denoised_dataset_path = \"./Dataset/1/dataset/denoised/\"\n",
    "treatments_output_path = './Dataset/1/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics checks for image classifications\n",
    "print(\"executing tensorflow version \" + tf.__version__)\n",
    "\n",
    "if (len(tf.config.experimental.list_physical_devices('GPU')) == 1):\n",
    "    print(\"GPU is detected\")\n",
    "else :\n",
    "    print(\"GPU isn't detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable 2 - Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the dataset (amount of images per batch, image resolution and training percentage)\n",
    "batch_size = 16\n",
    "img_height = 200\n",
    "img_width = 200\n",
    "validation_split = 0.2\n",
    "\n",
    "epochs=6\n",
    "classes = ['Photo', 'Other']\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def generate_model():\n",
    "    #generation of the training dataset\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    #generation of the validation dataset\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      classification_dataset_path,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    #retrieve the amount of classes for the model\n",
    "    num_classes = len(train_ds.class_names)\n",
    "    print(\"Classes found : \" + str(num_classes))\n",
    "    print(train_ds.class_names)\n",
    "\n",
    "    #Allow for perfomance compilation times by preventing IO bottleneck on disks while compiling the model\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #Structure of the neural network\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.Conv2D(16, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(64, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dropout(0.2),\n",
    "      layers.Dense(2)\n",
    "    ])\n",
    "    \n",
    "    #display neural network structure\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy', get_f1])\n",
    "    \n",
    "    #amount of training and fitting\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "    \n",
    "    #display statitics over training accuracy\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def classify_image(model, classes, impath):\n",
    "    #load disj image\n",
    "    img = image.load_img((impath), target_size=(img_height, img_width))\n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "\n",
    "    #use model to predict classe\n",
    "    prediction = model.predict(img)\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    #     print(\n",
    "    #         \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    #         .format(classes[np.argmax(score)], 100 * np.max(score))\n",
    "    #     )\n",
    "    #return clas and percentage of confidence\n",
    "    return [classes[np.argmax(score)], 100 * np.max(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(newModel=False):\n",
    "    # Checking if the Models folder is empty\n",
    "    if newModel == True: \n",
    "        model = generate_model()\n",
    "        tf.keras.models.save_model(model, classification_model_path)\n",
    "        return model    \n",
    "    # Or not\n",
    "    else:\n",
    "        return tf.keras.models.load_model(classification_model_path)\n",
    "    \n",
    "model = get_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence threshold about consider a image as a photo\n",
    "confidence_threshold = 90\n",
    "\n",
    "# Classify images and moved \"Photo\" to the output folder \n",
    "for pictures in os.listdir(classification_input_path):\n",
    "    res_classe, res_score = classify_image(model, classes, os.path.join(classification_input_path,pictures))\n",
    "    \n",
    "    print(pictures, res_classe, res_score, '%')\n",
    "\n",
    "    if (res_classe == 'Photo' and res_score > confidence_threshold):\n",
    "        shutil.copy2(classification_input_path + \"/\" + pictures, classification_output_path) \n",
    "\n",
    "# Single image test\n",
    "#img_name = 'photo_0001.jpg'\n",
    "#result = classify_image(model, classes, './Dataset/2/input/'+img_name)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - Prétraitement (denoising/sharpening…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblurring function\n",
    "def remove_blur(img, high):\n",
    "    kernel = []\n",
    "    \n",
    "    if high:\n",
    "        # Creation of a Laplacian kernel to use for debluring\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    else:\n",
    "        kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    \n",
    "    # Convolution of the kernel with the image given in the function's parameter\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def get_blurry_indicator(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    return fm\n",
    "\n",
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = blurry_dataset_path + name\n",
    "    img = get_image(blurry_dataset_path,name)\n",
    "\n",
    "     # Get initial Blur metric\n",
    "    original_blur_metric = get_blurry_indicator(img)\n",
    "    pre_processed_data.append(original_blur_metric)\n",
    "    \n",
    "    # Remove blur from the colored image image\n",
    "    deblurred_img = remove_blur(img, high=True)\n",
    "\n",
    "    # Get initial Blur metric\n",
    "    processed_blur_metric = get_blurry_indicator(deblurred_img)\n",
    "    post_processed_data.append(processed_blur_metric)\n",
    "    \n",
    "    #print(\"image \" + name + \" - initial : \" + str(original_blur_metric)\n",
    "     #   + \" - processed : \" + str(processed_blur_metric)\n",
    "     #   + \" - difference : \" + str(processed_blur_metric - original_blur_metric)+\"\\n\")\n",
    "\n",
    "    data_preview_blurr.append([name, original_blur_metric, processed_blur_metric])\n",
    "\n",
    "    # Saving Image\n",
    "    save_image(deblured_dataset_path, name, deblurred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(blurry_dataset_path)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_blurr = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_blurr)\n",
    "    \n",
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "def display_image_diff(originalPath, diffPath, filename=None):\n",
    "    if not filename:\n",
    "        # Get a random file from directory\n",
    "        filename = random.choice(os.listdir(originalPath)) \n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(get_image(originalPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    # Corrected Image noise\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(get_image(diffPath, filename))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Corrected Image\")\n",
    "    \n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(blurry_dataset_path, deblured_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Noise function\n",
    "def remove_noise(image, high):\n",
    "    if high == 2:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "    elif high == 1:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 5, 10, 7, 15)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 15)\n",
    "\n",
    "def estimate_noise(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return estimate_sigma(img)\n",
    "\n",
    "# Thread execution\n",
    "def process_fpath(name):\n",
    "    path = noisy_dataset_path + name\n",
    "    img = get_image(noisy_dataset_path,name)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    original_noise_metric = estimate_noise(img)\n",
    "    pre_processed_data.append(original_noise_metric)\n",
    "    \n",
    "    denoised_img = remove_noise(img, high=2)\n",
    "    \n",
    "    # Get initial noise metric\n",
    "    processed_noise_metric = estimate_noise(denoised_img)\n",
    "    post_processed_data.append(processed_noise_metric)\n",
    "\n",
    "    data_preview_denoised.append([name, original_noise_metric, processed_noise_metric])\n",
    "    \n",
    "    save_image(denoised_dataset_path, name, denoised_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of files to treat\n",
    "listing = os.listdir(noisy_dataset_path)\n",
    "\n",
    "# Loop on the list of file\n",
    "threads = []\n",
    "pre_processed_data = []\n",
    "post_processed_data = []\n",
    "data_preview_denoised = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in listing:\n",
    "        #process_fpath(name)\n",
    "        t = threading.Thread(target=process_fpath, args=(name,))\n",
    "        threads.append(t)\n",
    "        \n",
    "    # Start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    get_metric_stat(pre_processed_data, post_processed_data)\n",
    "    get_list_data(data_preview_denoised)\n",
    "    \n",
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# Filename MUST be the same for both directories    \n",
    "display_image_diff(noisy_dataset_path, denoised_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation entre Défloutage et Débruitage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_image(img):\n",
    "    \n",
    "    #if you want random testing\n",
    "    #img = get_image(\"./Dataset/Blurry/\", random.choice(os.listdir(\"./Dataset/Blurry/\")))\n",
    "\n",
    "    #initial image measurements\n",
    "    initial_noise = estimate_noise(img)\n",
    "    initial_blur = get_blurry_indicator(img)\n",
    "    # print(initial_noise, initial_blur)\n",
    "\n",
    "    #image is blurry\n",
    "    if initial_blur < 3000:\n",
    "        # high deblur of the image\n",
    "        img = remove_blur(img, high=False)\n",
    "\n",
    "        #second image measurements\n",
    "        second_noise = estimate_noise(img)\n",
    "        second_blur = get_blurry_indicator(img)\n",
    "\n",
    "        #image doesn't meets requirements in terms of noise\n",
    "        if second_noise > 1:\n",
    "            #low denoise of the image\n",
    "            img = remove_noise(img, 0)\n",
    "\n",
    "            img = remove_noise(img, 0)\n",
    "\n",
    "    #if image is noisy      \n",
    "    if initial_noise > 1:\n",
    "        # high denoise of the image\n",
    "        img = remove_noise(img, 2)\n",
    "\n",
    "        img = remove_noise(img, 1)\n",
    "\n",
    "        #second image measurements\n",
    "        second_noise = estimate_noise(img)\n",
    "        second_blur = get_blurry_indicator(img)\n",
    "\n",
    "        #image meets requirements in terms of blur\n",
    "        if second_blur < 48000:\n",
    "            #low deblur of the image\n",
    "            img = remove_blur(img, high=False)\n",
    "            img = remove_blur(img, high=False)\n",
    "            \n",
    "    print('Image cleaning is done ;)')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pictures in os.listdir(classification_output_path):\n",
    "    c_img = clear_image(img)\n",
    "    \n",
    "    #displaying images\n",
    "    plt.figure(figsize=(24, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    hutil.copy2(classification_output_path + \"/\" + pictures, treatments_output_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3 - Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "        \n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picture_to_output(img, result):\n",
    "    # Remove the ''<end>' from the titles\n",
    "    result = filter(lambda x: x != '<end>', result)\n",
    "    # Remove the ''<unk>' from the titles\n",
    "    result = filter(lambda x: x != '<unk>', result)\n",
    "    # Remove the '\\n' from the titles\n",
    "    result = filter(lambda x: x != '\\n', result)\n",
    "    \n",
    "    # Move the image with his result as name to output folder\n",
    "    dest_path = captioning_output_path + '/' + img + ' - ' + ' '.join(result) + '.jpg'\n",
    "    shutil.copy(image_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the pictures into the \"input\" folder\n",
    "for img in os.listdir(treatments_output_path):\n",
    "    image_path = treatments_output_path + '/' + img\n",
    "    \n",
    "    # Evaluate the given image\n",
    "    result, attention_plot = evaluate(image_path)\n",
    "    print(img, 'Prediction Caption:', ' '.join(result))\n",
    "\n",
    "    # Give the details of the word founded\n",
    "    # plot_attention(image_path, result, attention_plot)\n",
    "\n",
    "    # Save image into output folder\n",
    "    save_picture_to_output(img, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
